<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Transformers and GPT: An In-depth Overview | Hui Jiang </title> <meta name="author" content="Hui Jiang"> <meta name="description" content="In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/huijiang/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/huijiang/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/huijiang/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/huijiang/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://incml.github.io/huijiang/blog/2023/Transformer-GPT/"> <script src="/huijiang/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/huijiang/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/huijiang/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/huijiang/assets/js/distillpub/template.v2.js"></script> <script src="/huijiang/assets/js/distillpub/transforms.v2.js"></script> <script src="/huijiang/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding Transformers and GPT: An In-depth Overview",
            "description": "In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture.",
            "published": "March 05, 2023",
            "authors": [
              
              {
                "author": "Hui Jiang",
                "authorURL": "https://www.cse.yorku.ca/~huijiang",
                "affiliations": [
                  {
                    "name": "York University, Toronto, Canada",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/huijiang//"> <span class="font-weight-bold">Hui</span> Jiang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/huijiang/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/huijiang/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/students/">students </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding Transformers and GPT: An In-depth Overview</h1> <p>In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#transformers-forward-pass">Transformers: Forward Pass</a> </div> <div> <a href="#transformers-backward-pass">Transformers: Backward Pass</a> </div> <div> <a href="#gpt-3">GPT-3</a> </div> <ul> <li> <a href="#i-estimating-clean-image">I. Estimating clean image</a> </li> <li> <a href="#ii-estimating-noise">II. Estimating noise</a> </li> </ul> <div> <a href="#final-remarks">Final Remarks</a> </div> </nav> </d-contents> <p>Transformers <d-cite key="vaswani2023attentionneed"></d-cite> are a type of neural network architecture designed to transform a sequence of \(T\) input vectors,</p> \[\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T \} \;\;\;\;\;(\mathbf{x}_i \in \mathbb{R}^d, \; \forall i=1,2,\cdots,T),\] <p>into an equal-length sequence of the so-called context-dependent output vectors:</p> \[\{ \mathbf{y}_1, \mathbf{y}_2, \cdots, \mathbf{y}_T \} \;\;\;\;\;(\mathbf{y}_i \in \mathbb{R}^h, \; \forall i=1,2,\cdots,T).\] <p>The output sequence in a transformer model is referred to as <em>context-dependent</em> because each output vector is influenced not only by the corresponding input vector but also by the context of the entire input sequence. Specifically, each output vector \(\mathbf{y}_i\) depends on all input vectors in the sequence, not just \(\mathbf{x}_i\) at the same position. As a result, each output vector can be viewed as a representation of not only the input vector at the same location but also its contextual information in the entire sequence.</p> <p>More importantly, transformers utilize a flexible attention mechanism that enables them to generate each output vector \(\mathbf{y}_i\) in a way that mainly relies on a certain number of the most relevant input vectors from anywhere in the input sequence, rather than just those input vectors near the position \(i\) . This ability to selectively attend to relevant information in the input sequence allows transformers to capture long-range dependencies and contextual information, making them a powerful tool for natural language processing and other sequential data tasks.</p> <h2 id="transformers-forward-pass"><strong>Transformers: Forward Pass</strong></h2> <p>Letâ€™s pack all input vectors as a \(d \times T\) matrix, and all output vectors as an \(l \times T\) matrix, as follows:</p> \[\mathbf{X} = \bigg[ \mathbf{x}_1 \; \mathbf{x}_2 \; \cdots \; \mathbf{x}_T \bigg]_{d \times T}\] \[\mathbf{Y} = \bigg[ \mathbf{y}_1 \; \mathbf{y}_2 \; \cdots \; \mathbf{y}_T \bigg]_{h \times T}\] <p>In this way, a transformer can be viewed as a function \(\cal{T}\) that maps from \(\mathbf{X}\) to \(\mathbf{Y}\):</p> \[\cal{T}: \;\;\; \mathbf{X} \longrightarrow \mathbf{Y}\] <p>In the following, we will investigate all steps in the above mapping in a transformer.</p> <ul> <li> <strong>Forward step 1:</strong> we first introduce three parameter matrices \(\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{h \times d}\), which transform each input vector \(\mathbf{x}_i\) to generate the so-called <em>query</em> vector \(\mathbf{q}_i\), <em>key</em> vector \(\mathbf{k}_i\), and <em>value</em> vector \(\mathbf{v}_i\):</li> </ul> \[\mathbf{q}_i = \mathbf{A} \mathbf{x}_i, \; \mathbf{k}_i = \mathbf{B} \mathbf{x}_i, \; \mathbf{v}_i = \mathbf{C} \mathbf{x}_i \;\;\;\;(\forall i =1,2, \cdots,T)\] <p>When the <em>query</em>, <em>key</em>, and <em>value</em> vectors are all derived from a common input source, we refer to the transformerâ€™s mechanism as performing <em>self-attention</em>. Conversely, if these vectors are derived from different sources, the mechanism is called <em>cross-attention</em>.</p> <p>The above operations can be combined as three matrix multiplications in the following:</p> \[\mathbf{Q} = \mathbf{A} \mathbf{X}, \;\; \mathbf{K} = \mathbf{B} \mathbf{X}, \;\; \mathbf{V} = \mathbf{C} \mathbf{X}\] <p>where \(\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{h \times T}\) are constructed by lining up the above vectors \(\mathbf{q}_i\), \(\mathbf{k}_i\) and \(\mathbf{v}_i\) column by column.</p> <ul> <li> <strong>Forward step 2:</strong> use the above <em>query</em> and <em>key</em> vectors to compute all pair-wise attention between any two input vectors \(\mathbf{x}_i\) and \(\mathbf{x}_t\) (\(\forall i,t =1,2, \cdots, T\)) as follows:</li> </ul> \[c_{it} = \frac{\mathbf{q}_i^\intercal \mathbf{k}_t}{\sqrt{h}} \;\;\;\;\;\;\;\; (\forall i,t =1,2, \cdots, T)\] <p>Next, we normalize each \(c_{it}\) with respect to all \(i\) using the <em>softmax</em> function as follows:</p> \[a_{it} = \frac{e^{c_{it}}}{\sum_{j=1}^T \; e^{c_{jt}}} \;\;\;\;\;\;\;\;(\forall i,t =1,2, \cdots, T)\] <p>We can pack all operations in this step into the following matrix operation:</p> \[\mathcal{A} = \textrm{softmax}\big(\mathbf{Q}^\intercal \mathbf{K}/\sqrt{h} \big) \;\;\;\;\;\; (\mathcal{A} \in \mathbb{R}^{T \times T})\] <p>where the <em>softmax</em> operation is applied to the underlying matrix column-wise.</p> <ul> <li> <strong>Forward step 3:</strong> use the above $a_{it}$ as the attention coeffificents to generate an context-dependent vector from all <em>value</em> vectors:</li> </ul> \[\mathbf{z}_t = \sum_{i=1}^T \; a_{it} \mathbf{v}_i \;\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] <p>We can represent the above as the following matrix multiplication:</p> \[\mathbf{Z} = \mathbf{V} \mathcal{A} = \mathbf{V} \; \textrm{softmax}\big(\mathbf{Q}^\intercal \mathbf{K}/\sqrt{h} \big)\] <p>When transformers are employed as decoders to produce tokens, we typically utilize a form of attention known as causal attention. This attention mechanism ensures that each output vector is influenced solely by the input vectors that precede it, rather than any vectors that appear later in the sequence. In this case, we generate each $\mathbf{z}_t$ as</p> \[\mathbf{z}_t = \sum_{i=1}^t \; a_{it} \mathbf{v}_i \;\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] <p>To compute causal attention in matrix form, an upper-triangular matrix is employed to mask the attention matrix $\mathcal{A}$. This masking ensures that the attention mechanism only attends to previous positions in the sequence, as represented by the upper-triangular elements of the attention matrix, while ignoring future positions represented by the lower-triangular elements.</p> <ul> <li> <strong>Forward step 4:</strong> apply the layer normalization and one layer of fully-connected feedforward neural network to each $\mathbf{z}_t$ to generate the final output vector $\mathbf{y}_t$ as follows (note that residual connections <d-cite key="he2015deepresiduallearningimage"></d-cite> are introduced here to facilitate optimization during learning):</li> </ul> \[\bar{\mathbf{z}}_t = \mathbf{x}_t + \textrm{LN}_{\mathbf{X} + \gamma,\beta} \big( \mathbf{z}_t \big)\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] \[\mathbf{y}_t = \bar{\mathbf{z}}_t + \textrm{feedforward} \big( \bar{\mathbf{z}}_t \big) = \bar{\mathbf{z}}_t + \mathbf{W}_2 \; \textrm{ReLU}( \mathbf{W}_1 \bar{\mathbf{z}}_t ) \;\;\;(\forall t = 1,2,\cdots,T)\] <p>where two more parameter matrices $\mathbf{W}_1 \in \mathbb{R}^{hâ€™ \times h}$ and $\mathbf{W}_2 \in \mathbb{R}^{h \times hâ€™}$ are introduced here. For convenience, we can use the following compact matrix form to represent all operations in this step:</p> \[\mathbf{Y} = \mathbf{X} + \textrm{LN}_{\gamma,\beta} \big( \mathbf{Z} \big) +\textrm{feedforward} \Big( \mathbf{X} + \textrm{LN}_{\gamma,\beta} \big( \mathbf{Z} \big)\Big).\] <p>In summary, we can illustrate all attention operations in the forward pass of a transformer using matrices as in Figure 1.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-480.webp 480w,/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-800.webp 800w,/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 1. An illustration of all attention operations (steps 1, 2 and 3) in a transformer. </div> <h2 id="transformers-backward-pass"><strong>Transformers: Backward Pass</strong></h2> <p>Let us now examine how to perform the backward pass to propagate errors in a transformer as well as how to compute the gradients of all transformer parameter matrices, specifically for all attention operations illustrated in Figure 1. For the backward pass of layer normalization and fully-connected feedforward layer, readers are directed to section 8.3.2 in reference <d-cite key="JiangMLF2021"></d-cite>.</p> <p>Assuming we possess error signals for the transformer outputs with respect to a particular objective function $F(\cdot)$, these signals are given as follows:</p> \[\mathbf{e}_t \overset{\Delta}{=} \frac{\partial F}{ \partial \mathbf{z}_t} \;\;\;\;\;\; (\forall t = 1, 2, \cdots, T)\] <p>Arrange them column by column as a matrix:</p> \[\mathbf{E} = \frac{\partial F}{\partial \mathbf{Z}} = \left[ \;\; \frac{\partial F}{ \partial \mathbf{z}_t} \;\; \right]_{h \times T}\] <p>Letâ€™s break down all attention operations in a transformer in Figure 1 step by step backwards from the output towards input as follows:</p> <ul> <li> <strong>Backward step 1:</strong> we have</li> </ul> \[\mathbf{z}_t = \sum_{i=1}^T a_{it} \, \mathbf{v}_i\;\;\;\;\;\;\;\;\;\; (\forall t = 1,2, \cdots, T)\] <p>According to the chain rule, we can compute</p> \[\frac{\partial F}{\partial a_{it} } = \frac{\partial F}{\partial \mathbf{z}_t } \frac{\partial \mathbf{z}_t}{ \partial a_{it}} = \mathbf{v}_i^\intercal \frac{\partial F}{\partial \mathbf{z}_t } \;\;\;\;\;(\forall i,t=1,2,\cdots,T)\] <p>Align all of these ($T^2$ terms in total) as a $T \times T$ matrix:</p> \[\left[ \;\; \frac{\partial F}{\partial a_{it}} \;\; \right]_{T \times T } = \bigg[ \;\; \mathbf{V}^\intercal \;\; \bigg]_{T\times h } \bigg[ \;\; \mathbf{E} \;\; \bigg]_{h \times T }\] <ul> <li> <strong>Backward step 2:</strong> we normalize as $a_{it} = \frac{e^{c_{i}t}}{\sum_{j=1}^T e^{c_{jt}}} \;\;\;\;\;\; (\forall i,t = 1,2, \cdots, T)$.</li> </ul> <p>We denote</p> \[\mathbf{a}_t \overset{\Delta}{=} \big[ a_{1t} \, a_{2t} \, \cdots, a_{Tt} \big]^\intercal \;\;\;\;\; \textrm{and} \;\;\;\; \mathbf{c}_t \overset{\Delta}{=} \big[ c_{1t} \, c_{2t} \, \cdots, c_{Tt} \big]^\intercal\] \[\frac{\partial F}{\partial \mathbf{a}_t } \overset{\Delta}{=} \big[ \frac{\partial F}{\partial a_{1t}} \, \frac{\partial F}{\partial a_{2t}} \, \cdots \, \frac{\partial F}{\partial a_{Tt}} \big]^\intercal \;\;\;\;\; \textrm{and} \;\;\;\; \frac{\partial F}{\partial \mathbf{c}_t } \overset{\Delta}{=} \big[ \frac{\partial F}{\partial c_{1t}} \, \frac{\partial F}{\partial c_{2t}} \, \cdots \, \frac{\partial F}{\partial c_{Tt}} \big]^\intercal\] <p>According to Eq.(8.14) on page 180 in $[3]$, for any $t=1,2,\cdots,T$, we have</p> \[\frac{\partial F}{\partial \mathbf{c}_t } = \mathbf{J}_{\tiny sm}(t) \; \frac{\partial F}{\partial \mathbf{a}_t } \;\;\;\;\; (\forall t=1,2,\cdots,T)\] <p>with $\mathbf{J}_{\tiny sm}(t) = \textrm{diag} \big( \mathbf{a}_t \big) - \mathbf{a}_t \mathbf{a}_t^\intercal$. Furthermore, we use vector inner products to simplify the above matrix multiplications as follows:</p> \[\frac{\partial F}{\partial \mathbf{c}_t } = \mathbf{J}_{\tiny sm}(t) \; \frac{\partial F}{\partial \mathbf{a}_t } = \Big( \textrm{diag} \big( \mathbf{a}_t \big) - \mathbf{a}_t \mathbf{a}_t^\intercal \Big) \; \frac{\partial F}{\partial \mathbf{a}_t } = \mathbf{a}_t \odot \frac{\partial F}{\partial \mathbf{a}_t } - \big ( \mathbf{a}_t^\intercal \frac{\partial F}{\partial \mathbf{a}_t } \big)\mathbf{a}_t \;\;\;(\forall t=1,2,\cdots,T)\] <p>where $\odot$ indicates element-wise multiplication of two vectors. Next, we align the above results column by column for all $t=1,2,\cdots, T$ and use the notation $\otimes$ to indicate the batch of all $T$ above operations as follows:</p> \[\left[ \;\;\;\; \frac{\partial F}{\partial \mathbf{c}_t } \;\;\;\; \right]_{T \times T } = \mathcal{A} \otimes \left[ \;\;\;\; \frac{\partial F}{\partial \mathbf{a}_t} \;\;\;\; \right]_{T \times T }\] <p>It is worth noting that the aforementioned backward implementation can be applied directly to <em>causal attention</em> without any modifications.</p> <ul> <li> <strong>Backward step 3:</strong> due to $c_{it} = \mathbf{q}_i^\intercal \mathbf{k}_t/\sqrt{h} \;\;\;\;\;\; (\forall i,t = 1,2,\cdots, T)$, we have</li> </ul> \[\frac{\partial F}{ \partial \mathbf{q}_i} = \sum_{t=1}^T \frac{\partial F}{\partial c_{it} } \frac{\partial c_{it}}{\partial \mathbf{q}_i} = \frac{1}{\sqrt{h}}\sum_{t=1}^T \frac{\partial F}{\partial c_{it} } \mathbf{k}_t\] <p>Align these vectors column by column as the following matrix format:</p> \[\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} = \frac{1}{\sqrt{h}} \bigg[ \;\; \mathbf{K} \;\; \bigg]_{h \times T} \left[ \;\; \frac{\partial F}{\partial \mathbf{c}_t } \;\; \right]^\intercal_{T \times T }\] <ul> <li> <strong>Backward step 4:</strong> because of $\mathbf{q}_i = \mathbf{\mathbf{A}} \mathbf{x}_i$, we have</li> </ul> \[\frac{\partial F}{\partial \mathbf{A}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{q}_i} \frac{\partial \mathbf{q}_i}{\partial \mathbf{A}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{q}_i} \mathbf{x}_i^\intercal = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] <p>Putting all the above steps together, we have</p> \[\frac{\partial F}{\partial \mathbf{A}} = \frac{1}{\sqrt{h}}\bigg[ \;\; \mathbf{K} \;\; \bigg]_{h \times T} \Bigg( \mathcal{A} \otimes \Bigg( \bigg[ \;\; \mathbf{V}^\intercal \;\; \bigg]_{T\times h } \bigg[ \;\; \mathbf{E} \;\; \bigg]_{h \times T } \Bigg) \Bigg)^\intercal \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] \[= \frac{1}{\sqrt{h}} \mathbf{K} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg)^\intercal \mathbf{X}^\intercal\] <p>Similarly, we can derive</p> \[\frac{\partial F}{\partial \mathbf{B}} = \frac{1}{\sqrt{h}} \mathbf{Q} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg) \mathbf{X}^\intercal\] <p>Since we have</p> \[\mathbf{z}_t = \sum_{i=1}^T a_{it} \mathbf{v}_i\;\; (\forall t = 1,2, \cdots, T)\] <p>We compute</p> \[\frac{\partial F}{ \partial \mathbf{v}_i} = \sum_{t=1}^T a_{it} \frac{\partial F}{\partial \mathbf{z}_t}\] <p>Arrange them column by column into a matrix</p> \[\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\;\bigg]_{h \times T} = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{z}_i} \;\;\bigg]_{h \times T} \bigg[ \;\; a_{it} \;\; \bigg]^\intercal_{T \times T} = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{z}_i} \;\;\bigg]_{h \times T} \mathcal{A}^\intercal = \mathbf{E} \mathcal{A}^\intercal\] <p>Since we have $\mathbf{v}_i = \mathbf{C} \mathbf{x}_i \;\; (\forall i=1,2,\cdots,T)$ , we compute</p> \[\frac{\partial F}{\partial \mathbf{C}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{v}_i} \frac{\partial \mathbf{v}_i}{\partial \mathbf{C}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{v}_i} \mathbf{x}_i^\intercal = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\; \bigg]_{h \times T} \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] <p>As a result, we have</p> \[\frac{\partial F}{\partial \mathbf{C }} = \mathbf{E} \mathcal{A}^\intercal \mathbf{X}^\intercal\] <p>Finally, we back-propagate the error signals from output to input. The input $\mathbf{X}$ affects the output through three different paths, i.e. $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$. Therefore, we have</p> \[\frac{\partial F}{\partial \mathbf{X}} = \frac{\partial F}{\partial \mathbf{Q}} \frac{\partial \mathbf{Q}}{ \partial \mathbf{X}} + \frac{\partial F}{\partial \mathbf{K}} \frac{\partial \mathbf{K}}{ \partial \mathbf{X}} + \frac{\partial F}{\partial \mathbf{V}} \frac{\partial \mathbf{V}}{ \partial \mathbf{X}}\] \[= \bigg[ \;\; \mathbf{A}^\intercal \;\; \bigg]_{d\times h} \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} + \bigg[ \;\; \mathbf{B}^\intercal \;\; \bigg]_{d\times h} \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{k}_i} \;\; \bigg]_{h \times T} + \bigg[ \;\; \mathbf{C}^\intercal \;\; \bigg]_{d\times l}\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\; \bigg]_{h \times T}\] \[= \frac{1}{\sqrt{h}} \bigg( \mathbf{A}^\intercal \mathbf{K} \Big( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \Big)^\intercal + \mathbf{B}^\intercal \mathbf{Q} \Big( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \Big) \bigg) + \mathbf{C}^\intercal \mathbf{E} \mathcal{A}^\intercal\] <p>Finally, we summarize the above results using a more compact matrix representation. If we define the following $3h \times T$ matrix:</p> \[\mathbf{P} \overset{\Delta}{=} \begin{bmatrix} \frac{1}{\sqrt{h}} \mathbf{K} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg)^\intercal \\ \frac{1}{\sqrt{h}} \mathbf{Q} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg) \\ \mathbf{E} \mathcal{A}^\intercal \end{bmatrix}_{ 3h \times T}\] <p>we have</p> \[\begin{bmatrix} \frac{\partial F}{\partial \mathbf{A}} \\[0.1cm] \frac{\partial F}{\partial \mathbf{B}} \\[0.1cm] \frac{\partial F}{\partial \mathbf{C}} \end{bmatrix} = \mathbf{P} \, \mathbf{X}^\intercal \;\;\;\;\;\;\Big(\in \mathbb{R}^{3h \times d} \Big)\] \[\frac{\partial F}{\partial \mathbf{X}} = \begin{bmatrix} \mathbf{A} \\ \mathbf{B} \\ \mathbf{C} \end{bmatrix}^\intercal \, \mathbf{P} = \Big[ \mathbf{A}^\intercal \;\; \mathbf{B}^\intercal \;\; \mathbf{C}^\intercal \Big] \, \mathbf{P} \;\;\;\;\;\;\Big(\in \mathbb{R}^{d \times T} \Big)\] <p><em>Note: Refer to <a href="https://colab.research.google.com/drive/1-op--04cwJI2L8iPNIgNoPsJ3uPke8OB" rel="external nofollow noopener" target="_blank">a pytorch implementation at Colab</a> and its comparison with pytorch autograd (or a <a href="https://colab.research.google.com/drive/1WOdo0AuSn-lIzOxXLxTwDAKPJhzEvJ2e" rel="external nofollow noopener" target="_blank">JAX implementation at Colab</a>).</em></p> <h2 id="gpt-3"><strong>GPT-3</strong></h2> <p>The authors of <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite> propose a deep multi-head transformer structure named <em>GPT-3</em> for processing text data. In GPT-3, the values of $d$, $h$, and $T$ are set to $d=12288$, $h=128$, and $T=2048$, respectively, while the vocabulary is composed of $50257$ distinct tokens.</p> <p>To begin, <em>GPT-3</em> employs a tokenizer to split any text sequence into a sequence of tokens (each token is a common word fragment). These tokens are then transformed into vectors of $d=12288$ dimensions using a word embedding matrix $\mathbf{W}_0 \in \mathbb{R}^{12288 \times 50257}$. Subsequently, the input sequence $\mathbf{X} \in \mathbb{R}^{12288 \times 2048}$ is transformed into $\mathbf{Y} \in \mathbb{R}^{12288 \times 2048}$ using 96 layers of multi-head transformer blocks. Each block is defined as follows:</p> <p><strong>(1)</strong> Each multi-head transformer in <em>GPT-3</em> uses $96$ heads:</p> \[\mathbf{A}^{(j)}, \mathbf{B}^{(j)}, \mathbf{C}^{(j)} \in \mathbb{R}^{128 \times 12288} \;\;\; (j=1,2, \cdots, 96)\] <p>which compute for all $j=1,2,\cdots,T$:</p> \[\mathbf{Z}^{(j)} \in \mathbb{R}^{ 128 \times 2048} = \big( \mathbf{C}^{(j)} \mathbf{X} \big) \; \textrm{softmax}\Big( \big(\mathbf{A}^{(j)} \mathbf{X} \big)^\intercal \big( \mathbf{B}^{(j)} \mathbf{X} \big)/\sqrt{128}\Big)\] <p><strong>(2)</strong> Concatenate the outputs from all heads:</p> \[\mathbf{Z} \in \mathbb{R}^{12288 \times 2048} = \mathbf{W}^o \textrm{concat}\big(\mathbf{Z}^{(1)}, \cdots, \mathbf{Z}^{(96)}\big)\] <p>where $\mathbf{W}^o \in \mathbb{R}^{12288\times12288}$.</p> <p><strong>(3)</strong> Apply layer normalization to each column of $\mathbf{Z}$: $\mathbf{z}_t \in \mathbb{R}^{12288} \; (\forall t=1,2,\cdots,2048)$ as</p> \[\bar{\mathbf{z}}_t = \mathbf{x}_t + \textrm{LN}_{\gamma,\beta} \big(\mathbf{z}_t \big)\] <p><strong>(4)</strong> Apply nonlinearity to each column as:</p> \[\mathbf{y}_t= \bar{\mathbf{z}}_t + \textrm{feedforward} \big( \bar{\mathbf{z}}_t\big) = \bar{\mathbf{z}}_t + \mathbf{W}_2 \textrm{ReLU} (\mathbf{W_1} \bar{\mathbf{z}}_t)\] <p>where $\mathbf{W}_1 \in \mathbb{R}^{49152 \times 12288}$, and $\mathbf{W}_2 \in \mathbb{R}^{12288 \times 49152}$.</p> <p>Based on these, we may calculate that the total number of parameters in <em>GTP-3</em> is about $175$ billions.</p> <p>During training, a sequence of training vectors</p> \[\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_{2048} \}\] <p>is fed into <em>GPT-3</em> as input $\mathbf{X} \in \mathbb{R}^{12288\times 2048}$. For each time step $t=1,2,\cdots,2047$, <em>GPT-3</em> is trained to predict the token at position $t+1$ based on all vectors appearing up to position $t$, i.e., ${\mathbf{x}_1, \cdots, \mathbf{x}_t}$.</p> <p>After its training, GPT-3 has the ability to create new sequences by using an input sequence as a prompt. To do this, the model calculates the probabilities of the possible next tokens that could follow the given prompt, and then selects a new token by randomly sampling from these probabilities. The selected token is then added to the end of the prompt, forming a new prompt. This process continues until the model generates a termination token.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/huijiang/assets/bibliography/2023-03-05-Transformer-GPT.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Hui Jiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/huijiang/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/huijiang/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/huijiang/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/huijiang/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/huijiang/blog/"}},{id:"nav-publications",title:"publications",description:"a selection of my recent publications (since 2014) by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/huijiang/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/huijiang/projects/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/huijiang/repositories/"}},{id:"nav-teaching",title:"teaching",description:"List of courses I taught at York University.",section:"Navigation",handler:()=>{window.location.href="/huijiang/teaching/"}},{id:"nav-students",title:"students",description:"a list of all my current and former graduate students and postdocs",section:"Navigation",handler:()=>{window.location.href="/huijiang/students/"}},{id:"post-a-deterministic-view-of-diffusion-models",title:"A Deterministic View of Diffusion Models",description:"In this post, we present a deterministic perspective on diffusion models. In this approach, neural networks are trained as an inverse function of the deterministic diffusion mapping that progressively corrupts images at each time step. This method simplifies the derivation of diffusion models, enabling us to fully explain and derive them using only a few straightforward mathematical equations.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2024/Deterministic-Diffusion-Models/"}},{id:"post-understanding-transformers-and-gpt-an-in-depth-overview",title:"Understanding Transformers and GPT: An In-depth Overview",description:"In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2023/Transformer-GPT/"}},{id:"post-machine-learning-fundamentals",title:"Machine Learning Fundamentals",description:"Supplementary materials for the book &quot;Machine Learning Fundamentals&quot;, published by Cambridge University Press.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2022/Machine-Learning-Fundamentals/"}},{id:"news-my-new-book-machine-learning-fundamentals-is-published",title:"My new book \u201cMachine Learning Fundamentals\u201d is published.",description:"",section:"News",handler:()=>{window.location.href="/huijiang/news/announcement_0/"}},{id:"news-my-personal-website-is-moved-here-from-yorku-eecs-server-https-wiki-eecs-yorku-ca-user-hj",title:"My personal website is moved here from [YorkU EECS server](https://wiki.eecs.yorku.ca/user/hj/).",description:"",section:"News"},{id:"projects-deterministic-diffussion-models",title:"Deterministic Diffussion Models",description:"This project redirects to a Jupyter Notebook at Google Colab for study and experiments.",section:"Projects",handler:()=>{window.location.href="/huijiang/projects/project_DDM/"}},{id:"projects-machine-learning-fundamentals",title:"Machine Learning Fundamentals",description:"\xa9Hui Jiang 2021, Cambridge University Press",section:"Projects",handler:()=>{window.location.href="/huijiang/projects/project_MLF/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%75%69%6A%69%61%6E%67@%79%6F%72%6B%75.%63%61","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=lQi05ZkAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/36357862","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Hui-Jiang-13/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/iNCML","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hui-jiang-8b860630","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/huijiang313","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/huijiang/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>