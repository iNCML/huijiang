<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Natural Languages as ε-Ambiguity Languages | Hui Jiang </title> <meta name="author" content="Hui Jiang"> <meta name="description" content="In this post, we examines the claim that natural languages are ε-ambiguity languages in the sense defined by the probabilistic theories of language and latent-intention inference in (Jiang, 2023). This article surveys linguistic, psycholinguistic, and computational evidence demonstrating that natural languages exhibit precisely this structure."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/huijiang/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/huijiang/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/huijiang/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/huijiang/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://incml.github.io/huijiang/blog/2026/epsilon-language/"> <script src="/huijiang/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/huijiang/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/huijiang/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/huijiang/assets/js/distillpub/template.v2.js"></script> <script src="/huijiang/assets/js/distillpub/transforms.v2.js"></script> <script src="/huijiang/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Natural Languages as ε-Ambiguity Languages",
            "description": "In this post, we examines the claim that natural languages are ε-ambiguity languages in the sense defined by the probabilistic theories of language and latent-intention inference in (Jiang, 2023). This article surveys linguistic, psycholinguistic, and computational evidence demonstrating that natural languages exhibit precisely this structure.",
            "published": "January 12, 2026",
            "authors": [
              
              {
                "author": "Hui Jiang",
                "authorURL": "https://www.cse.yorku.ca/~huijiang",
                "affiliations": [
                  {
                    "name": "York University, Toronto, Canada",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/huijiang//"> <span class="font-weight-bold">Hui</span> Jiang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/huijiang/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/huijiang/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/huijiang/students/">students </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Natural Languages as ε-Ambiguity Languages</h1> <p>In this post, we examines the claim that natural languages are ε-ambiguity languages in the sense defined by the probabilistic theories of language and latent-intention inference in (Jiang, 2023). This article surveys linguistic, psycholinguistic, and computational evidence demonstrating that natural languages exhibit precisely this structure.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-%CE%B5-ambiguity-framework">The ε-Ambiguity Framework</a> </div> <div> <a href="#evidence-from-psycholinguistics">Evidence from Psycholinguistics</a> </div> <div> <a href="#evidence-from-computational-linguistics">Evidence from Computational Linguistics</a> </div> <div> <a href="#theoretical-justification-from-information-theory">Theoretical Justification from Information Theory</a> </div> <div> <a href="#bayesian-models-of-language-support-%CE%B5-ambiguity">Bayesian Models of Language Support ε-Ambiguity</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="the-ε-ambiguity-framework"><strong>The ε-Ambiguity Framework</strong></h2> <p>Natural languages support reliable communication despite variability, noise, and structural underspecification. Unlike programming languages, they allow metaphor, ellipsis, ambiguity, deixis, and context-dependent meaning. Yet humans typically recover the intended meaning with high accuracy.</p> <p>Recent theoretical work <d-cite key="Jiang2023LatentSpaceEmergent"></d-cite> introduces <strong>ε-ambiguity languages</strong> as a formal tool to model this phenomenon. Under this framework, a language is ε-ambiguous if, for any meaningful message <em>x</em>, there exists a dominant intended meaning θ₀ such that</p> \[\Pr(\theta_0 \mid x) \ge 1 - \varepsilon(x), \quad \varepsilon(x) \in [0,1),\] <p>but alternative interpretations occur with small but non-zero probability.</p> <p>In this framework, although many meanings are technically compatible with a linguistic expression, one meaning dominates the posterior probability, and ambiguity occurs only with small probability ε(x). In <d-cite key="Jiang2023LatentSpaceEmergent"></d-cite>, it argues that natural languages empirically exhibit ε-ambiguity, namely they are neither perfectly unambiguous nor fully ambiguous, but instead allow for reliably dominant meanings with bounded ambiguity. Furthermore, <d-cite key="Jiang2023LatentSpaceEmergent"></d-cite> argues that ε-ambiguity provides a coherent explanation for both human semantics and LLM emergent abilities.</p> <h3 id="model-definition">Model Definition</h3> <p>As in <d-cite key="Jiang2023LatentSpaceEmergent"></d-cite>, we assume:</p> <ul> <li>A latent intention space Θ,</li> <li>A surface linguistic expression <em>x</em> generated via a distribution q(x ∣ θ) with θ ∈ Θ.</li> <li>A listener infers meaning via the posterior probability Pr(θ ∣ x).</li> </ul> <p>A language is an <strong>ε-ambiguity language</strong> if for every meaningful expression x:</p> \[\Pr(\theta_0 \mid x) \ge 1 - \varepsilon(x),\] <p>where θ₀ is the intended meaning, and ε(x) quantifies residual ambiguity.</p> <p>This describes a <em>sparse posterior</em> over meanings: a dominant intention and a long but very small tail of alternatives.</p> <h3 id="consequences">Consequences</h3> <p>This model predicts that:</p> <ol> <li>Communication is reliable but not deterministic.</li> <li>Ambiguity decreases <strong>multiplicatively</strong> when multiple cues or messages are provided.</li> <li>Latent-intention inference is feasible even without explicit symbolic structure.</li> </ol> <p>The fundamental principle behind ε-ambiguity languages is that linguistic expressions exhibit <em>partial but not perfect</em> semantic determinacy. Messages tend to convey one meaning with high probability, but never with absolute certainty. Real-world natural languages possess exactly these properties: they support efficient communication despite intrinsic ambiguity, and ambiguity is controlled by contextual, semantic, and pragmatic mechanisms.</p> <p>The ε-ambiguity framework formalizes this intuition within a probabilistic generative model of communication, where meanings (latent intentions θ) are drawn from a space Θ, and surface messages are generated by noisy processes with intention-specific distributions q(x ∣ θ). The framework provides a mathematical explanation for why LLMs can infer hidden meanings from text and why phenomena such as chain-of-thought reasoning reduce uncertainty.</p> <p>The central question of this article is:</p> <blockquote> <p><strong>What empirical and theoretical evidence supports the view that natural languages satisfy the definition of ε-ambiguity languages?</strong></p> </blockquote> <p>We demonstrate that evidence comes from multiple research domains.</p> <hr> <h2 id="evidence-from-linguistics"><strong>Evidence from Linguistics</strong></h2> <h3 id="lexical-ambiguity-and-polysemy">Lexical Ambiguity and Polysemy</h3> <p>Natural languages contain extensive lexical ambiguity <d-cite key="Lyons1977Semantics"></d-cite> <d-cite key="Cruse1986LexicalSemantics"></d-cite>. Words often possess multiple senses (e.g., <em>bank</em>, <em>seal</em>, <em>interest</em>), yet human speakers reliably infer the dominant meaning from context. Corpus-based studies of word sense disambiguation have shown that sense distributions are often highly skewed, with the most frequent sense accounting for a large majority of occurrences <d-cite key="Kilgarriff1992Polysemy"></d-cite> <d-cite key="Yarowsky1993OneSensePerCollocation"></d-cite> <d-cite key="Kilgarriff2004DominantSense"></d-cite>.</p> <p>This illustrates exactly the condition:</p> \[\Pr(\theta_0 \mid x) \approx 1 - \varepsilon(x), \quad \varepsilon(x) \text{ small but nonzero},\] <p>where θ₀ is the dominant sense.</p> <h3 id="syntactic-ambiguity">Syntactic Ambiguity</h3> <p>Classic syntactic ambiguities (e.g., <em>“I saw the man with the telescope”</em>) allow multiple parses, yet listeners overwhelmingly adopt one interpretation when context is provided. Probabilistic grammars assign steeply skewed probability distributions to parses <d-cite key="CharniakJohnson2005CoarseToFine"></d-cite>, again demonstrating nonzero but concentrated posterior distributions over intentions.</p> <h3 id="pragmatic-inference-and-speech-acts">Pragmatic Inference and Speech Acts</h3> <p>Pragmatics often shifts literal meanings to intended meanings <d-cite key="Grice1975LogicConversation"></d-cite>. For instance:</p> <ul> <li> <em>“Can you pass the salt?”</em> is interpreted as a request, not a question about capability.</li> </ul> <p>Research on speech-act recognition <d-cite key="Austin1962HowToDoThings"></d-cite><d-cite key="Searle1969SpeechActs"></d-cite> shows that listeners infer intended acts with high reliability but occasional errors—consistent with ε &gt; 0.</p> <hr> <h2 id="evidence-from-psycholinguistics"><strong>Evidence from Psycholinguistics</strong></h2> <h3 id="rapid-probabilistic-disambiguation">Rapid Probabilistic Disambiguation</h3> <p>Humans use contextual probabilities to resolve ambiguity almost instantaneously <d-cite key="Tanenhaus1995VisualLinguisticIntegration"></d-cite>. Even in garden-path sentences (e.g., <em>“The horse raced past the barn fell”</em>), misinterpretations occur but are rare relative to successful parsing.</p> <p>This supports the claim that:</p> <ul> <li>A single meaning θ₀ dominates in the human posterior belief,</li> <li>But alternate meanings retain small probability mass <d-cite key="TraxlerPickering2003SyntacticAmbiguity"></d-cite>.</li> </ul> <h3 id="context-dependence-and-disambiguation">Context-Dependence and Disambiguation</h3> <p>Work in contextual integration <d-cite key="MacDonaldPearlmutterSeidenberg1994LexicalAmbiguity"></d-cite> shows that humans update interpretations probabilistically as context accumulates, consistent with the multiplicative reduction in ambiguity predicted by ε-ambiguity theory, i.e. Proposition 1 in <d-cite key="Jiang2023LatentSpaceEmergent"></d-cite>.</p> <p>The ε-ambiguity model predicts (and proves) that when a listener receives multiple messages \((x_1, x_2, \dots, x_m )\) generated from the same θ:</p> \[\varepsilon_{\text{combined}} \approx \varepsilon(x_1)\varepsilon(x_2)\cdots\varepsilon(x_m).\] <p>This is aligned with psychological evidence that humans aggregate cues. This multiplicative decay of ε explains:</p> <ul> <li>why conversation repair is effective,</li> <li>why adding context sharpens meaning,</li> </ul> <p>Thus ε plays the role in controlling inference quality.</p> <h3 id="repair-mechanisms">Repair Mechanisms</h3> <p>Conversation analysis <d-cite key="SchegloffJeffersonSacks1977SelfCorrection"></d-cite> shows that misunderstandings occur at low but non-zero frequency, and repair strategies efficiently correct them—suggesting that ε(x) is generally small but salient.</p> <h3 id="cooperative-principle">Cooperative Principle</h3> <p>Grice’s cooperative principle <d-cite key="Grice1975LogicConversation"></d-cite> ensures that interpretation leans toward meanings that maximize communicative coherence, forcing</p> \[\Pr(\theta_0 \mid x) \gg \Pr(\theta\_{\text{alt}} \mid x),\] <p>even when multiple interpretations are technically possible.</p> <hr> <h2 id="evidence-from-computational-linguistics"><strong>Evidence from Computational Linguistics</strong></h2> <h3 id="corpus-based-skew-of-meaning-distributions">Corpus-Based Skew of Meaning Distributions</h3> <p>Probabilistic models such as topic models, PCFGs, and neural parsers show extreme sparsity in the joint distribution of meanings and linguistic forms <d-cite key="ManningSchutze1999StatNLP"></d-cite>. For example:</p> <ul> <li>One parse typically has probability ≫ 0.9,</li> <li>Alternate parses share the remaining mass.</li> </ul> <p>This sparsity is exactly the structure assumed for ε-ambiguous languages.</p> <h3 id="behavior-of-large-language-models">Behavior of Large Language Models</h3> <p>LLMs themselves reveal ε-like behavior:</p> <h4 id="sensitivity-to-prompt-ambiguity">Sensitivity to Prompt Ambiguity</h4> <p>When prompts are under-specified, LLM outputs diverge, demonstrating non-zero ε(x). When prompts are clarified or expanded (e.g., chain-of-thought prompting), the model’s output variance collapses—interpretable as effective ε(x) decreasing multiplicatively with additional linguistic evidence <d-cite key="Wei2022ChainOfThought"></d-cite>.</p> <h4 id="convergence-under-contextual-redundancy">Convergence Under Contextual Redundancy</h4> <p>The theoretical models show that concatenating independent messages reduces ambiguity roughly like:</p> \[\varepsilon_{\text{combined}} \approx \prod_i \varepsilon(x_i).\] <p>This aligns with empirical improvements when LLMs receive:</p> <ul> <li>multiple examples in in-context learning <d-cite key="Brown2020FewShot"></d-cite>,</li> <li>multiple paraphrases of a query <d-cite key="Gao2021BetterFewShot"></d-cite>,</li> <li>deliberate step-by-step reasoning <d-cite key="Wang2022SelfConsistency"></d-cite>.</li> </ul> <h4 id="improved-performance-with-additional-cues">Improved Performance with Additional Cues</h4> <p>Work on instruction tuning <d-cite key="Ouyang2022InstructRLHF"></d-cite> shows that LLMs improve dramatically when intentions are expressed more explicitly; implicit or ambiguous instructions (large ε) yield errors.</p> <hr> <h2 id="theoretical-justification-from-information-theory"><strong>Theoretical Justification from Information Theory</strong></h2> <p>The empirical observations in previous sections strongly suggest that natural languages align with the ε-ambiguity formalism. In this section, we present a more formal theoretical justification supporting the necessity of ε-ambiguity for any human language capable of large-scale communication, inference, and compositional generalization.</p> <h3 id="communication-under-uncertainty-requires-controlled-ambiguity">Communication Under Uncertainty Requires Controlled Ambiguity</h3> <p>Let θ denote a latent intention and x a surface linguistic signal. Human communication is characterized by:</p> <ol> <li> <p><strong>Inherent variability in production</strong><br> Speakers do not produce perfectly deterministic signals for intentions.</p> \[H(x \mid \theta) &gt; 0.\] </li> <li> <p><strong>Redundancy and recoverability in comprehension</strong><br> Listeners consistently recover the intended meaning despite variability:</p> \[\Pr(\theta_0 \mid x) \text{ is typically high}.\] </li> </ol> <p>This requires that the conditional distribution over intentions, q(θ ∣ x), be <strong>sharply peaked</strong> but not a delta distribution. Formally, this implies:</p> \[\Pr(\theta_0 \mid x) = 1 - \varepsilon(x),\] <p>where ε(x) captures the intrinsic noise or ambiguity.</p> <p>If ε(x) were zero:</p> <ul> <li>Language would be fully deterministic, like a programming language.</li> <li>Yet natural languages support ellipsis, metaphor, deixis, and underspecification—contradicting determinism.</li> </ul> <p>If ε(x) were large:</p> <ul> <li>Communication would be unreliable and human languages could not function as coordination tools <d-cite key="Lewis1969Convention"></d-cite>.</li> </ul> <p>Thus, human language must live in the intermediate regime:</p> \[0 &lt; \varepsilon(x) \ll 1.\] <p>This is precisely the definition of an ε-ambiguity language.</p> <h3 id="information-theoretic-justification">Information-Theoretic Justification</h3> <p>Given Shannon’s channel coding theorem, any efficient communication system must satisfy:</p> \[H(\theta \mid x) &gt; 0,\] <p>unless compressed messages carry arbitrarily large complexity.</p> <p>Natural languages are highly compressed representations of latent intentions. For compression to be efficient:</p> \[H(\theta \mid x) = \mathbb{E}[\varepsilon(x)] &gt; 0\] <p>must hold. But for communication to function at all:</p> \[H(\theta \mid x) \ll H(\theta)\] <p>must also hold.</p> <p>This yields:</p> \[0 &lt; \varepsilon(x) \ll 1.\] <p>Thus ε is not merely empirical—it is <strong>forced</strong> by fundamental information-theoretic constraints on communication between bounded agents.</p> <h3 id="ambiguity-as-a-structural-requirement-for-expressivity">Ambiguity as a Structural Requirement for Expressivity</h3> <p>A classic result from information theory states that, under bounded channel capacity, a communication code must balance:</p> <ul> <li>expressivity (large hypothesis space Θ),</li> <li>efficiency (short messages),</li> <li>robustness (recoverability under noise).</li> </ul> <p>Natural languages accomplish this by encoding intentions in <strong>probabilistic distributions over many correlated cues</strong> (syntax, semantics, prosody, discourse), with none individually deterministic. This “multi-cue redundancy” structure implies that:</p> \[q(x \mid \theta) \text{ is broad, but structured},\] <p>leading again to:</p> \[\Pr(\theta_0 \mid x) \approx 1 - \varepsilon(x)\] <p>for some small ε(x).</p> <p>The ε term mathematically captures the trade-off between:</p> <ul> <li>variability (which increases expressive richness), and</li> <li>stability (needed for mutual intelligibility).</li> </ul> <p>Thus <strong>ε-ambiguity is not an accident</strong> but a structural necessity for language to be both expressive and learnable.</p> <h3 id="ambiguity-is-necessary-for-compositionality">Ambiguity Is Necessary for Compositionality</h3> <p>A fully deterministic mapping from θ → x (ε = 0) would break compositionality in languages such as English:</p> <ul> <li>Metaphor, ellipsis, and deixis require listeners to infer missing structure.</li> <li>Rich morphology and syntax permit underspecified constructions that rely on inference rather than explicit specification.</li> </ul> <p>If ε were zero, all linguistic constructions would require exhaustive specification of intentions, leading to:</p> <ul> <li>infinitely long messages,</li> <li>no pragmatic inference,</li> <li>no flexible reference resolution.</li> </ul> <p>Conversely, if ε is small but non-zero, compositional structures can afford <strong>underspecification</strong>, because the listener’s inferential machinery resolves them with high probability.</p> <p>Thus ε &gt; 0 is a prerequisite for <strong>efficient and human-like generative grammar</strong>.</p> <hr> <h2 id="bayesian-models-of-language-support-ε-ambiguity"><strong>Bayesian Models of Language Support ε-Ambiguity</strong></h2> <p>Probabilistic pragmatics <d-cite key="GoodmanFrank2016PragmaticInference"></d-cite> models the listener as:</p> \[\Pr(\theta \mid x) \propto \Pr(x \mid \theta)\Pr(\theta).\] <p>Empirically, these models consistently find:</p> <ul> <li>a dominant intention θ₀ that captures most posterior mass</li> <li>a long tail of alternative intentions with total mass ε(x)</li> </ul> <p>Thus:</p> \[\Pr(\theta_0 \mid x) = 1 - \varepsilon(x)\] <p>emerges naturally as a mathematical property of Bayesian interpretation under realistic priors and likelihoods.</p> <p>This shows that <strong>ε-ambiguity is a mathematically inevitable property of any communicative system interpreted via Bayesian reasoning</strong>, which includes both humans and LLMs.</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>Across linguistic, psycholinguistic, computational, and conversational evidence, natural languages display the following features:</p> <table> <thead> <tr> <th>Property</th> <th>Observed in Natural Languages</th> <th>Matches ε-Ambiguous Definition</th> </tr> </thead> <tbody> <tr> <td>Multiple interpretations possible</td> <td>✔</td> <td>ε(x) &gt; 0</td> </tr> <tr> <td>One interpretation strongly dominant</td> <td>✔</td> <td>Pr(θ₀ | x ) ≈ 1-ε(x)</td> </tr> <tr> <td>Ambiguity decreases with context</td> <td>✔</td> <td>ε(x₁x₂) ≈ ε(x₁)ε(x₂)</td> </tr> <tr> <td>Communication is highly reliable</td> <td>✔</td> <td>ε(x) generally small</td> </tr> <tr> <td>Occasional misinterpretation occurs</td> <td>✔</td> <td>ε(x) nonzero</td> </tr> </tbody> </table> <p>Natural languages therefore satisfy the fundamental requirements of ε-ambiguity languages: they encode meaning with probabilistic stability but not absolute determinacy.</p> <p>The ε-ambiguity model elegantly captures the essential structure of natural language semantics. Evidence from multiple disciplines demonstrates that natural languages exhibit:</p> <ul> <li>constrained ambiguity,</li> <li>dominant interpretations,</li> <li>context-driven reduction of uncertainty,</li> <li>non-zero but bounded error rates.</li> </ul> <p>At last, the following empirical facts also align closely with the ε-ambiguity framework and serve as the further evidence:</p> <ul> <li>Human communication requires nonzero uncertainty, but that uncertainty must be bounded ⟹ ε(x) &gt; 0 but small.</li> <li>Efficient, expressive languages require underspecification, which in turn requires ε(x) ≠ 0.</li> <li>Bayesian inference over meanings implies skewed but non-delta posterior distributions ⟹ the ε form.</li> <li>Multiple evidence aggregation reduces uncertainty multiplicatively, as predicted by the ε-ambiguity formalism.</li> <li>Shannon’s information theory forces 0&lt;ε≪1 in any efficient coding system.</li> <li>Psycholinguistic data shows humans behave exactly like ε-ambiguity decoders.</li> </ul> <p>Together, these points provide a rigorous foundation for treating natural languages as ε-ambiguity languages.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/huijiang/assets/bibliography/2026-01-12-epsilon-language.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Hui Jiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/huijiang/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/huijiang/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/huijiang/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/huijiang/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/huijiang/blog/"}},{id:"nav-publications",title:"publications",description:"a selection of my recent publications (since 2014) by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/huijiang/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/huijiang/projects/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/huijiang/repositories/"}},{id:"nav-teaching",title:"teaching",description:"List of courses I taught at York University.",section:"Navigation",handler:()=>{window.location.href="/huijiang/teaching/"}},{id:"nav-students",title:"students",description:"a list of all my current and former graduate students and postdocs",section:"Navigation",handler:()=>{window.location.href="/huijiang/students/"}},{id:"post-natural-languages-as-\u03b5-ambiguity-languages",title:"Natural Languages as \u03b5-Ambiguity Languages",description:"In this post, we examines the claim that natural languages are \u03b5-ambiguity languages in the sense defined by the probabilistic theories of language and latent-intention inference in (Jiang, 2023). This article surveys linguistic, psycholinguistic, and computational evidence demonstrating that natural languages exhibit precisely this structure.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2026/epsilon-language/"}},{id:"post-a-brief-introduction-to-bitcoin",title:"A Brief Introduction to Bitcoin",description:"In this post, we provide a concise introduction to the key technologies that underpin the Bitcoin network. This overview highlights several important implementations within Bitcoin and offers readers the foundational knowledge needed to understand how the network operates.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2025/Bitcoin/"}},{id:"post-a-deterministic-view-of-diffusion-models",title:"A Deterministic View of Diffusion Models",description:"In this post, we present a deterministic perspective on diffusion models. In this approach, neural networks are trained as an inverse function of the deterministic diffusion mapping that progressively corrupts images at each time step. This method simplifies the derivation of diffusion models, enabling us to fully explain and derive them using only a few straightforward mathematical equations.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2024/Deterministic-Diffusion-Models/"}},{id:"post-understanding-transformers-and-gpt-an-in-depth-overview",title:"Understanding Transformers and GPT: An In-depth Overview",description:"In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2023/Transformer-GPT/"}},{id:"post-machine-learning-fundamentals",title:"Machine Learning Fundamentals",description:"Supplementary materials for the book &quot;Machine Learning Fundamentals&quot;, published by Cambridge University Press.",section:"Posts",handler:()=>{window.location.href="/huijiang/blog/2022/Machine-Learning-Fundamentals/"}},{id:"news-my-new-book-machine-learning-fundamentals-is-published",title:"My new book \u201cMachine Learning Fundamentals\u201d is published.",description:"",section:"News",handler:()=>{window.location.href="/huijiang/news/announcement_0/"}},{id:"news-my-personal-website-is-moved-here-from-yorku-eecs-server-https-wiki-eecs-yorku-ca-user-hj",title:"My personal website is moved here from [YorkU EECS server](https://wiki.eecs.yorku.ca/user/hj/).",description:"",section:"News"},{id:"projects-deterministic-diffussion-models",title:"Deterministic Diffussion Models",description:"This project redirects to a Jupyter Notebook at Google Colab for study and experiments.",section:"Projects",handler:()=>{window.location.href="/huijiang/projects/project_DDM/"}},{id:"projects-machine-learning-fundamentals",title:"Machine Learning Fundamentals",description:"\xa9Hui Jiang 2021, Cambridge University Press",section:"Projects",handler:()=>{window.location.href="/huijiang/projects/project_MLF/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%75%69%6A%69%61%6E%67@%79%6F%72%6B%75.%63%61","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=lQi05ZkAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/36357862","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Hui-Jiang-13/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/iNCML","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hui-jiang-8b860630","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/huijiang313","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/huijiang/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>