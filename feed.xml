<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://incml.github.io/huijiang/feed.xml" rel="self" type="application/atom+xml"/><link href="https://incml.github.io/huijiang/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-17T16:33:47+00:00</updated><id>https://incml.github.io/huijiang/feed.xml</id><title type="html">blank</title><subtitle>Prof. Hui Jiang&apos;s personal website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Deterministic View of Diffusion Models</title><link href="https://incml.github.io/huijiang/blog/2024/Deterministic-Diffusion-Models/" rel="alternate" type="text/html" title="A Deterministic View of Diffusion Models"/><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2024/Deterministic-Diffusion-Models</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2024/Deterministic-Diffusion-Models/"><![CDATA[<p>In recent years, diffusion models, a novel category of deep generative models <d-cite key="JiangMLF2021"></d-cite>, have made significant strides in producing high-quality, high-resolution images. Notable examples include GLIDE <d-cite key="nichol2022glidephotorealisticimagegeneration"></d-cite>, DALLE-2 <d-cite key="ramesh2022hierarchicaltextconditionalimagegeneration"></d-cite>, Imagen <d-cite key="saharia2022photorealistictexttoimagediffusionmodels"></d-cite>, and the fully open-source Stable Diffusion <d-cite key="rombach2022highresolutionimagesynthesislatent"></d-cite>. These models are traditionally developed based on the framework of Denoising Diffusion Probabilistic Models (DDPM) <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite>. In this probabilistic framework, the forward diffusion process is modeled as a Gaussian process with Markovian properties. Conversely, the backward denoising process employs neural networks to estimate the conditional distribution at each time step. The neural networks involved in the denoising process are trained to minimize the evidence lower bound (ELBO) <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite>, akin to the approach used in a Variational Autoencoder (VAE) <d-cite key="Kingma_2019"></d-cite>.</p> <p>In this post, we present a deterministic perspective on diffusion models. In this method, neural networks are constructed to function in the opposite way of a deterministic diffusion process that gradually deteriorates images over time. This training allows the neural networks to reconstruct or generate images by reversing the diffusion process without using any knowledge of stochastic process. This method simplifies the derivation of diffusion models, making the process more straightforward and comprehensible. Within this deterministic framework, diffusion models can be fully explained and derived from scratch using only a few straightforward mathematical equations, as shown in this concise, self-contained post. This approach requires only basic mathematical knowledge, eliminating the need for lengthy tutorials filled with hundreds of complex equations involving stochastic processes and probability distributions. <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite>.</p> <h2 id="deterministic-forward-diffusion-process"><strong>Deterministic Forward Diffusion Process</strong></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. A deterministic view of diffusion models in the forward diffusion process and the backward denoising transformation (Image adapted from <d-cite key="karagiannakos2022diffusionmodels"></d-cite>). </div> <p>In Figure 1, we illustrate a deterministic view of the typical diffusion process in diffusion models. Starting with any input clean image, denoted as \(\mathbf{x}_0\), the forward process incrementally corrupts the input image for each time step \(t=1,2,\cdots,T\). This corruption is achieved by progressively adding varying levels of Gaussian noises over time as</p> \[\mathbf{x}_t = \sqrt{\alpha_t } \mathbf{x}_{t-1} + \sqrt{1- \alpha_t } \, {\boldsymbol \epsilon}_t \;\;\; \forall t=1, 2, \cdots, T\] <p>adhering to a predefined noise schedule: \(\alpha_1, \alpha_2, \ldots, \alpha_T\), where the noise at each timestep is Gaussian, \({\boldsymbol \epsilon}_t \sim \mathcal{N}(0, \mathbf{I})\). This process gradually introduces more noise at each step, leading to a sequence of increasingly corrupted versions of the original image: \(\mathbf{x}_0 \to \mathbf{x}_1 \to \mathbf{x}_2 \to \cdots \to \mathbf{x}_T\). When \(T\) is large enough, the last image \(\mathbf{x}_T\) approaches to a Gaussian noise, i.e. \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\).</p> <p>Building on the so-called “nice property” outlined in <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite> <d-cite key="karagiannakos2022diffusionmodels"></d-cite>, the above diffusion process can be implemented much more efficiently. Rather than sampling a unique Gaussian noise at each time step, it is feasible to sample a single Gaussian noise, \(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\), and employ the subsequent formula to efficiently generate all the corrupted samples in one go (along the left-to-right red dash arrow in Figure 1):</p> \[\begin{align} \mathbf{x}_t = f(\mathbf{x}_{0}, {\boldsymbol \epsilon}, t) = \sqrt{\bar{\alpha}_t} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \, {\boldsymbol \epsilon} \;\;\; \forall t=1, 2, \cdots, T \label{eq-forward-deterministic} \end{align}\] <p>where \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\) and we have \(\bar{\alpha}_t \to 0\) as \(t \to T\).</p> <p>In the diffusion process described in Eq. (\ref{eq-forward-deterministic}), once the noise \({\boldsymbol \epsilon}\) is sampled, it is treated as constant for the whole diffusion process. Consequently, the transformation from the clean image \(\mathbf{x}_{0}\) to noisy images \(\mathbf{x}_{t}\) at each time step \(t\) can be regarded as a deterministic mapping, characterized by the above function \(\mathbf{x}_t = f(\mathbf{x}_{0}, {\boldsymbol \epsilon}, t)\).</p> <p>As shown in Figure 2, clean images are gradually converted into pure noises in the above deterministic diffusion process as \(t\) goes from \(0\) to \(T\). The method in Eq.(\ref{eq-forward-deterministic}) streamlines the process, making the generation of corrupted samples more straightforward and less computationally demanding.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2. The deterministic diffussion process of some images selected from the MNIST-Fashion dataset. </div> <h2 id="deterministic-backward-denoising-process"><strong>Deterministic Backward Denoising Process</strong></h2> <p>If the forward diffusion process is treated as deterministic, the corresponding deterministic mappings for the backward denoising process – transforming any noisy image \(\mathbf{x}_{t}\) back to the clean image \(\mathbf{x}_{0}\) – can also be derived.</p> <p>Here, let’s explore the relationship between \(\mathbf{x}_{t-1}\) and \(\mathbf{x}_t\) in the above diffusion process. This exploration will help us understand how consecutive stages in the diffusion process are linked by a deterministic function, which is essential for the subsequent deterministic denoising methods. In fact, it is possible to establish this deterministic function connecting two consecutive samples using two different approaches.</p> <p><strong>(1)</strong> In the first method, assuming the noise \({\boldsymbol \epsilon}\) is known, we can rearrange eq.(\ref{eq-forward-deterministic}) for the time step, \(t\), as follows:</p> \[\begin{align} \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \big[ \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, {\boldsymbol \epsilon} \big] \label{eq-forwar-t} \end{align}\] <p>Furthermore, according to Eq.(\ref{eq-forward-deterministic}), for the previous time step \(t-1\), we have the following:</p> \[\begin{align} \mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \label{eq-forwar-t-1} \end{align}\] <p>We may substitue Eq.(\ref{eq-forwar-t}) into the above equation to derive the first relationship between any two adjacent samples as follows:</p> \[\begin{align} \begin{aligned} \mathbf{x}_{t-1} &amp;= \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \\ &amp;= \frac{1}{\sqrt{\alpha_t}} \big[ \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, {\boldsymbol \epsilon} \big] + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \\ &amp;= \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) {\boldsymbol \epsilon} \Big] \end{aligned} \label{eq-consecutive-time-noise} \end{align}\] <p><strong>(2)</strong> Alternatively, assuming the original clean image \(\mathbf{x}_{0}\) is known, we can rearrange Eq.(\ref{eq-forward-deterministic}) as follows</p> \[{\boldsymbol \epsilon} = \frac{1}{\sqrt{1 - \bar{\alpha}_{t}}} \big[ \mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}\big]\] <p>and substitute \({\boldsymbol \epsilon}\) into Eq.(\ref{eq-forwar-t-1}), we have</p> \[\begin{aligned} \mathbf{x}_{t-1} &amp;= \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \\ &amp;= \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \frac{\sqrt{1 - \bar{\alpha}_{t-1}}}{\sqrt{1 - \bar{\alpha}_{t}}} \big[ \mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}\big] \\ \end{aligned}\] <p>If we denote</p> \[\bar{\gamma}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\] <p>we can simplify the above equation as follows:</p> \[\begin{align} \mathbf{x}_{t-1} = \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) \mathbf{x}_0 \label{eq-consecutive-time-x0} \end{align}\] <p>However, in practice, we cannot directly use Eq. (\ref{eq-consecutive-time-noise}) or Eq. (\ref{eq-consecutive-time-x0}) to derive \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_t\) during the backward denoising process, as neither the noise \({\boldsymbol \epsilon}\) nor the original clean image ​\(\mathbf{x}_0\) is known. The key insight of diffusion models is that neural networks can be trained to approximate the inverse of the deterministic mapping function \(\mathbf{x}_t = f(\mathbf{x}_{0},\boldsymbol \epsilon, t)\) in the forward process. By leveraging this inverse mapping, the learned neural networks can help to estimate either the noise \({\boldsymbol \epsilon}\) or the original clean image ​\(\mathbf{x}_0\) from a noisy image \(\mathbf{x}_t\). There are two approaches for training neural networks, denoted as \({\boldsymbol \theta}\), to approximate this inverse function:</p> <ol> <li> <p>Approximate the inverse mapping from \(\mathbf{x}_t\) to the clean image \(\mathbf{x}_0\), i.e. \(\mathbf{\hat x}_0 = f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\), allowing Eq. (\ref{eq-consecutive-time-x0}) to be applied for the backward denoising process. This inverse mapping is shown as the right-to-left red dash arrow in Figure 1.</p> </li> <li> <p>Approximate the inverse mapping from \(\mathbf{x}_t\) to the noise \({\boldsymbol \epsilon}\), i.e. \(\hat{\boldsymbol \epsilon} = g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\), enabling the use of Eq. (\ref{eq-consecutive-time-noise}) for the backward denoising process.</p> </li> </ol> <p>Next, we will briefly explore these two different types of deterministic diffusion models.</p> <h2 id="the-deterministic-diffusion-models"><strong>The Deterministic Diffusion Models</strong></h2> <p>In the backward process, starting from a Gaussian noise</p> \[\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\] <p>we may gradually recover all corrupted images backwards one by one until we obtain the initial clean image:</p> \[\mathbf{x}_T \to \mathbf{x}_{T-1} \to \mathbf{x}_{T-2} \to \cdots \to \mathbf{x}_1 \to \mathbf{x}_0\] <p>Alternatively, at any time step \(t\), given the corrupted image \(\mathbf{x}_t\), we may also directly estimate the original clean image \(\mathbf{x}_0\) based on \(\mathbf{x}_t\). If the estimate is good enough, we can terminate the backward denoising process at an earlier stage; Otherwise, we further denoise one time step backwards, i.e. deriving \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_t\). Based on \(\mathbf{x}_{t-1}\), we may derive a better estimate of the clean image \(\mathbf{x}_0\). This denoising process continues until we finally obtain a sufficiently good clean image \(\mathbf{x}_0\).</p> <p>In the above backward denoising process, to obtain a slightly cleaner image \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_{t}\) using either Eq. (\ref{eq-consecutive-time-x0}) or Eq. (\ref{eq-consecutive-time-noise}), we have two options for training neural networks to approximate the inverse mapping, as previously discussed.</p> <h3 id="i-estimating-clean-image-mathbfx_0"><strong>I. Estimating clean image \(\mathbf{x}_0\)</strong></h3> <p>In this case, we construct a deep neural network \(\boldsymbol \theta\) to approximate the inverse function of the above diffusion mapping \(\mathbf{x}_t = f(\mathbf{x}_0, {\boldsymbol \epsilon}, t)\), denoted as</p> \[\mathbf{\hat x}_0 = f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <p>which can recover a rough estimate of the clean image \(\hat{\mathbf{x}}_0\) from any \(\mathbf{x}_t\) (along the right-to-left red dash arrow in Figure 1). In this case, the neural network is learned by minimizing the following objective function over all training data in the training set \(\mathcal{D}\):</p> \[\begin{aligned} L_1({\boldsymbol \theta}) &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) - \mathbf{x}_0\Big)^2 \\ &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( f^{-1}_{\boldsymbol \theta} \big(\sqrt{\bar{\alpha}_t} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \, {\boldsymbol \epsilon}, t \big) - \mathbf{x}_0\Big)^2 \end{aligned}\] <p>Once the neural network \(\boldsymbol \theta\) has been trained, Eq. (\ref{eq-consecutive-time-x0}) can be used to perform the backward denoising process:</p> \[\begin{aligned} \mathbf{x}_{t-1} &amp;= \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) \hat{\mathbf{x}}_0 \\ &amp;= \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) \\ \end{aligned}\] <p>The corresponding sampling process to generate a new image can be described as follows:</p> <ul> <li>sample a Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\)</li> <li><strong>for</strong> \(t=T, T-1, \cdots, 1\): <ul> <li>use the trained neural network \({\boldsymbol \theta}\) to compute</li> </ul> \[\hat{\mathbf{x}}_0 = f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <ul> <li><strong>if</strong> \(\hat{\mathbf{x}}_0\) is stable <strong>or</strong> \(t=1\), return \(\hat{\mathbf{x}}_0\)</li> <li><strong>else</strong> denoise one step backward as</li> </ul> </li> </ul> \[\mathbf{x}_{t-1} = \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) \hat{\mathbf{x}}_0\] <p>In Figure 3, we have shown some sampling results from the MNIST-Fashion dataset through the above sampling algorithm via building neural networks to estimate clean images.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. Some sampling results are shown from the MNIST-Fashion dataset via building neural networks to estimate clean images. Every two lines represent one sampling example: the first line displays denoising samples at each timestep while the second line shows the estimated clean image at each timestep. </div> <h3 id="ii-estimating-noise-boldsymbol-epsilon"><strong>II. Estimating noise \({\boldsymbol \epsilon}\)</strong></h3> <p>In this case, we construct a deep neural network \(\boldsymbol \theta\) to approximate the inverse function via estimating the noise \({\boldsymbol \epsilon}\) from a corrupted image \(\mathbf{x}_t\) at each time step \(t\):</p> \[\hat{\boldsymbol \epsilon} = g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <p>This neural network is learned by minimizing the following objective function over all training data:</p> \[\begin{aligned} L_2({\boldsymbol \theta}) &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) - {\boldsymbol \epsilon}\Big)^2 \\ &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( g^{-1}_{\boldsymbol \theta} \big(\sqrt{\bar{\alpha}_t} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \, {\boldsymbol \epsilon}, t \big) - {\boldsymbol \epsilon}\Big)^2 \end{aligned}\] <p>Once the neural network is learned, we can use Eq.(\ref{eq-consecutive-time-noise}) to derive an estimate of \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_{t}\) as follows:</p> \[\begin{aligned} \mathbf{x}_{t-1} &amp;= \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) \hat{\boldsymbol \epsilon} \Big] \\ &amp;= \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) \Big] \end{aligned}\] <p>Similarly, the corresponding sampling process to generate a new image can be described as follows:</p> <ul> <li> <p>sample a Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\)</p> </li> <li> <p><strong>for</strong> \(t=T, T-1, \cdots, 1\):</p> <ul> <li>use the trained neural network \({\boldsymbol \theta}\) to compute:</li> </ul> \[\hat{\boldsymbol \epsilon} = g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <ul> <li>estimate clean image as in Eq.(\ref{eq-forwar-t}):</li> </ul> \[\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \big[ \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, \hat{\boldsymbol \epsilon} \big]\] <ul> <li> <p><strong>if</strong> \(\hat{\mathbf{x}}_0\) is stable <strong>or</strong> \(t=1\), return \(\hat{\mathbf{x}}_0\)</p> </li> <li> <p><strong>else</strong> denoise one step backward as</p> </li> </ul> \[\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) \hat{\boldsymbol \epsilon} \Big]\] </li> </ul> <p>In Figure 4, we have shown some sampling results from the MNIST-Fashion dataset via building neural networks to estimate noises through the above sampling algorithm.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4. Some sampling results are shown from the MNIST-Fashion dataset via building neural networks to estimate noises. Every two lines represent one sampling example: the first line displays denoising samples at each timestep while the second line shows the estimated clean image at each timestep. </div> <h2 id="final-remarks"><strong>Final Remarks</strong></h2> <p>In recent years, diffusion models have become increasingly significant in computer vision, emerging as the dominant models for images and videos across a wide range of real-world applications. However, the traditional approach to understanding diffusion models presents a steep learning curve, as it requires advanced mathematical knowledge in areas such as probability density estimation, stochastic process, and differential equation <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite>. In this post, we have introduced a new deterministic perspective on diffusion models, enabling us to fully explain and derive them using only a few straightforward mathematical equations. Moreover, we have found that the two methods presented in this post are essentially equivalent to the Denoising Diffusion Implicit Models (DDIMs) method discussed in <d-cite key="song2022denoisingdiffusionimplicitmodels"></d-cite>. However, our methods are derived through a much simpler and more intuitive approach compared to the procedure outlined in <d-cite key="song2022denoisingdiffusionimplicitmodels"></d-cite>.</p>]]></content><author><name>Hui Jiang</name></author><category term="computer-vision"/><category term="machine-learming"/><category term="AI"/><summary type="html"><![CDATA[In this post, we present a deterministic perspective on diffusion models. In this approach, neural networks are trained as an inverse function of the deterministic diffusion mapping that progressively corrupts images at each time step. This method simplifies the derivation of diffusion models, enabling us to fully explain and derive them using only a few straightforward mathematical equations.]]></summary></entry><entry><title type="html">Understanding Transformers and GPT: An In-depth Overview</title><link href="https://incml.github.io/huijiang/blog/2023/Transformer-GPT/" rel="alternate" type="text/html" title="Understanding Transformers and GPT: An In-depth Overview"/><published>2023-03-05T00:00:00+00:00</published><updated>2023-03-05T00:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2023/Transformer-GPT</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2023/Transformer-GPT/"><![CDATA[<p>Transformers <d-cite key="vaswani2023attentionneed"></d-cite> are a type of neural network architecture designed to transform a sequence of \(T\) input vectors,</p> \[\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T \} \;\;\;\;\;(\mathbf{x}_i \in \mathbb{R}^d, \; \forall i=1,2,\cdots,T),\] <p>into an equal-length sequence of the so-called context-dependent output vectors:</p> \[\{ \mathbf{y}_1, \mathbf{y}_2, \cdots, \mathbf{y}_T \} \;\;\;\;\;(\mathbf{y}_i \in \mathbb{R}^h, \; \forall i=1,2,\cdots,T).\] <p>The output sequence in a transformer model is referred to as <em>context-dependent</em> because each output vector is influenced not only by the corresponding input vector but also by the context of the entire input sequence. Specifically, each output vector \(\mathbf{y}_i\) depends on all input vectors in the sequence, not just \(\mathbf{x}_i\) at the same position. As a result, each output vector can be viewed as a representation of not only the input vector at the same location but also its contextual information in the entire sequence.</p> <p>More importantly, transformers utilize a flexible attention mechanism that enables them to generate each output vector \(\mathbf{y}_i\) in a way that mainly relies on a certain number of the most relevant input vectors from anywhere in the input sequence, rather than just those input vectors near the position \(i\) . This ability to selectively attend to relevant information in the input sequence allows transformers to capture long-range dependencies and contextual information, making them a powerful tool for natural language processing and other sequential data tasks.</p> <h2 id="transformers-forward-pass"><strong>Transformers: Forward Pass</strong></h2> <p>Let’s pack all input vectors as a \(d \times T\) matrix, and all output vectors as an \(l \times T\) matrix, as follows:</p> \[\mathbf{X} = \bigg[ \mathbf{x}_1 \; \mathbf{x}_2 \; \cdots \; \mathbf{x}_T \bigg]_{d \times T}\] \[\mathbf{Y} = \bigg[ \mathbf{y}_1 \; \mathbf{y}_2 \; \cdots \; \mathbf{y}_T \bigg]_{h \times T}\] <p>In this way, a transformer can be viewed as a function \(\cal{T}\) that maps from \(\mathbf{X}\) to \(\mathbf{Y}\):</p> \[\cal{T}: \;\;\; \mathbf{X} \longrightarrow \mathbf{Y}\] <p>In the following, we will investigate all steps in the above mapping in a transformer.</p> <ul> <li><strong>Forward step 1:</strong> we first introduce three parameter matrices \(\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{h \times d}\), which transform each input vector \(\mathbf{x}_i\) to generate the so-called <em>query</em> vector \(\mathbf{q}_i\), <em>key</em> vector \(\mathbf{k}_i\), and <em>value</em> vector \(\mathbf{v}_i\):</li> </ul> \[\mathbf{q}_i = \mathbf{A} \mathbf{x}_i, \; \mathbf{k}_i = \mathbf{B} \mathbf{x}_i, \; \mathbf{v}_i = \mathbf{C} \mathbf{x}_i \;\;\;\;(\forall i =1,2, \cdots,T)\] <p>When the <em>query</em>, <em>key</em>, and <em>value</em> vectors are all derived from a common input source, we refer to the transformer’s mechanism as performing <em>self-attention</em>. Conversely, if these vectors are derived from different sources, the mechanism is called <em>cross-attention</em>.</p> <p>The above operations can be combined as three matrix multiplications in the following:</p> \[\mathbf{Q} = \mathbf{A} \mathbf{X}, \;\; \mathbf{K} = \mathbf{B} \mathbf{X}, \;\; \mathbf{V} = \mathbf{C} \mathbf{X}\] <p>where \(\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{h \times T}\) are constructed by lining up the above vectors \(\mathbf{q}_i\), \(\mathbf{k}_i\) and \(\mathbf{v}_i\) column by column.</p> <ul> <li><strong>Forward step 2:</strong> use the above <em>query</em> and <em>key</em> vectors to compute all pair-wise attention between any two input vectors \(\mathbf{x}_i\) and \(\mathbf{x}_t\) (\(\forall i,t =1,2, \cdots, T\)) as follows:</li> </ul> \[c_{it} = \frac{\mathbf{q}_i^\intercal \mathbf{k}_t}{\sqrt{h}} \;\;\;\;\;\;\;\; (\forall i,t =1,2, \cdots, T)\] <p>Next, we normalize each \(c_{it}\) with respect to all \(i\) using the <em>softmax</em> function as follows:</p> \[a_{it} = \frac{e^{c_{it}}}{\sum_{j=1}^T \; e^{c_{jt}}} \;\;\;\;\;\;\;\;(\forall i,t =1,2, \cdots, T)\] <p>We can pack all operations in this step into the following matrix operation:</p> \[\mathcal{A} = \textrm{softmax}\big(\mathbf{Q}^\intercal \mathbf{K}/\sqrt{h} \big) \;\;\;\;\;\; (\mathcal{A} \in \mathbb{R}^{T \times T})\] <p>where the <em>softmax</em> operation is applied to the underlying matrix column-wise.</p> <ul> <li><strong>Forward step 3:</strong> use the above $a_{it}$ as the attention coeffificents to generate an context-dependent vector from all <em>value</em> vectors:</li> </ul> \[\mathbf{z}_t = \sum_{i=1}^T \; a_{it} \mathbf{v}_i \;\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] <p>We can represent the above as the following matrix multiplication:</p> \[\mathbf{Z} = \mathbf{V} \mathcal{A} = \mathbf{V} \; \textrm{softmax}\big(\mathbf{Q}^\intercal \mathbf{K}/\sqrt{h} \big)\] <p>When transformers are employed as decoders to produce tokens, we typically utilize a form of attention known as causal attention. This attention mechanism ensures that each output vector is influenced solely by the input vectors that precede it, rather than any vectors that appear later in the sequence. In this case, we generate each $\mathbf{z}_t$ as</p> \[\mathbf{z}_t = \sum_{i=1}^t \; a_{it} \mathbf{v}_i \;\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] <p>To compute causal attention in matrix form, an upper-triangular matrix is employed to mask the attention matrix $\mathcal{A}$. This masking ensures that the attention mechanism only attends to previous positions in the sequence, as represented by the upper-triangular elements of the attention matrix, while ignoring future positions represented by the lower-triangular elements.</p> <ul> <li><strong>Forward step 4:</strong> apply the layer normalization and one layer of fully-connected feedforward neural network to each $\mathbf{z}_t$ to generate the final output vector $\mathbf{y}_t$ as follows (note that residual connections <d-cite key="he2015deepresiduallearningimage"></d-cite> are introduced here to facilitate optimization during learning):</li> </ul> \[\bar{\mathbf{z}}_t = \mathbf{x}_t + \textrm{LN}_{\mathbf{X} + \gamma,\beta} \big( \mathbf{z}_t \big)\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] \[\mathbf{y}_t = \bar{\mathbf{z}}_t + \textrm{feedforward} \big( \bar{\mathbf{z}}_t \big) = \bar{\mathbf{z}}_t + \mathbf{W}_2 \; \textrm{ReLU}( \mathbf{W}_1 \bar{\mathbf{z}}_t ) \;\;\;(\forall t = 1,2,\cdots,T)\] <p>where two more parameter matrices $\mathbf{W}_1 \in \mathbb{R}^{h’ \times h}$ and $\mathbf{W}_2 \in \mathbb{R}^{h \times h’}$ are introduced here. For convenience, we can use the following compact matrix form to represent all operations in this step:</p> \[\mathbf{Y} = \mathbf{X} + \textrm{LN}_{\gamma,\beta} \big( \mathbf{Z} \big) +\textrm{feedforward} \Big( \mathbf{X} + \textrm{LN}_{\gamma,\beta} \big( \mathbf{Z} \big)\Big).\] <p>In summary, we can illustrate all attention operations in the forward pass of a transformer using matrices as in Figure 1.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-480.webp 480w,/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-800.webp 800w,/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. An illustration of all attention operations (steps 1, 2 and 3) in a transformer. </div> <h2 id="transformers-backward-pass"><strong>Transformers: Backward Pass</strong></h2> <p>Let us now examine how to perform the backward pass to propagate errors in a transformer as well as how to compute the gradients of all transformer parameter matrices, specifically for all attention operations illustrated in Figure 1. For the backward pass of layer normalization and fully-connected feedforward layer, readers are directed to section 8.3.2 in reference <d-cite key="JiangMLF2021"></d-cite>.</p> <p>Assuming we possess error signals for the transformer outputs with respect to a particular objective function $F(\cdot)$, these signals are given as follows:</p> \[\mathbf{e}_t \overset{\Delta}{=} \frac{\partial F}{ \partial \mathbf{z}_t} \;\;\;\;\;\; (\forall t = 1, 2, \cdots, T)\] <p>Arrange them column by column as a matrix:</p> \[\mathbf{E} = \frac{\partial F}{\partial \mathbf{Z}} = \left[ \;\; \frac{\partial F}{ \partial \mathbf{z}_t} \;\; \right]_{h \times T}\] <p>Let’s break down all attention operations in a transformer in Figure 1 step by step backwards from the output towards input as follows:</p> <ul> <li><strong>Backward step 1:</strong> we have</li> </ul> \[\mathbf{z}_t = \sum_{i=1}^T a_{it} \, \mathbf{v}_i\;\;\;\;\;\;\;\;\;\; (\forall t = 1,2, \cdots, T)\] <p>According to the chain rule, we can compute</p> \[\frac{\partial F}{\partial a_{it} } = \frac{\partial F}{\partial \mathbf{z}_t } \frac{\partial \mathbf{z}_t}{ \partial a_{it}} = \mathbf{v}_i^\intercal \frac{\partial F}{\partial \mathbf{z}_t } \;\;\;\;\;(\forall i,t=1,2,\cdots,T)\] <p>Align all of these ($T^2$ terms in total) as a $T \times T$ matrix:</p> \[\left[ \;\; \frac{\partial F}{\partial a_{it}} \;\; \right]_{T \times T } = \bigg[ \;\; \mathbf{V}^\intercal \;\; \bigg]_{T\times h } \bigg[ \;\; \mathbf{E} \;\; \bigg]_{h \times T }\] <ul> <li><strong>Backward step 2:</strong> we normalize as $a_{it} = \frac{e^{c_{i}t}}{\sum_{j=1}^T e^{c_{jt}}} \;\;\;\;\;\; (\forall i,t = 1,2, \cdots, T)$.</li> </ul> <p>We denote</p> \[\mathbf{a}_t \overset{\Delta}{=} \big[ a_{1t} \, a_{2t} \, \cdots, a_{Tt} \big]^\intercal \;\;\;\;\; \textrm{and} \;\;\;\; \mathbf{c}_t \overset{\Delta}{=} \big[ c_{1t} \, c_{2t} \, \cdots, c_{Tt} \big]^\intercal\] \[\frac{\partial F}{\partial \mathbf{a}_t } \overset{\Delta}{=} \big[ \frac{\partial F}{\partial a_{1t}} \, \frac{\partial F}{\partial a_{2t}} \, \cdots \, \frac{\partial F}{\partial a_{Tt}} \big]^\intercal \;\;\;\;\; \textrm{and} \;\;\;\; \frac{\partial F}{\partial \mathbf{c}_t } \overset{\Delta}{=} \big[ \frac{\partial F}{\partial c_{1t}} \, \frac{\partial F}{\partial c_{2t}} \, \cdots \, \frac{\partial F}{\partial c_{Tt}} \big]^\intercal\] <p>According to Eq.(8.14) on page 180 in $[3]$, for any $t=1,2,\cdots,T$, we have</p> \[\frac{\partial F}{\partial \mathbf{c}_t } = \mathbf{J}_{\tiny sm}(t) \; \frac{\partial F}{\partial \mathbf{a}_t } \;\;\;\;\; (\forall t=1,2,\cdots,T)\] <p>with $\mathbf{J}_{\tiny sm}(t) = \textrm{diag} \big( \mathbf{a}_t \big) - \mathbf{a}_t \mathbf{a}_t^\intercal$. Furthermore, we use vector inner products to simplify the above matrix multiplications as follows:</p> \[\frac{\partial F}{\partial \mathbf{c}_t } = \mathbf{J}_{\tiny sm}(t) \; \frac{\partial F}{\partial \mathbf{a}_t } = \Big( \textrm{diag} \big( \mathbf{a}_t \big) - \mathbf{a}_t \mathbf{a}_t^\intercal \Big) \; \frac{\partial F}{\partial \mathbf{a}_t } = \mathbf{a}_t \odot \frac{\partial F}{\partial \mathbf{a}_t } - \big ( \mathbf{a}_t^\intercal \frac{\partial F}{\partial \mathbf{a}_t } \big)\mathbf{a}_t \;\;\;(\forall t=1,2,\cdots,T)\] <p>where $\odot$ indicates element-wise multiplication of two vectors. Next, we align the above results column by column for all $t=1,2,\cdots, T$ and use the notation $\otimes$ to indicate the batch of all $T$ above operations as follows:</p> \[\left[ \;\;\;\; \frac{\partial F}{\partial \mathbf{c}_t } \;\;\;\; \right]_{T \times T } = \mathcal{A} \otimes \left[ \;\;\;\; \frac{\partial F}{\partial \mathbf{a}_t} \;\;\;\; \right]_{T \times T }\] <p>It is worth noting that the aforementioned backward implementation can be applied directly to <em>causal attention</em> without any modifications.</p> <ul> <li><strong>Backward step 3:</strong> due to $c_{it} = \mathbf{q}_i^\intercal \mathbf{k}_t/\sqrt{h} \;\;\;\;\;\; (\forall i,t = 1,2,\cdots, T)$, we have</li> </ul> \[\frac{\partial F}{ \partial \mathbf{q}_i} = \sum_{t=1}^T \frac{\partial F}{\partial c_{it} } \frac{\partial c_{it}}{\partial \mathbf{q}_i} = \frac{1}{\sqrt{h}}\sum_{t=1}^T \frac{\partial F}{\partial c_{it} } \mathbf{k}_t\] <p>Align these vectors column by column as the following matrix format:</p> \[\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} = \frac{1}{\sqrt{h}} \bigg[ \;\; \mathbf{K} \;\; \bigg]_{h \times T} \left[ \;\; \frac{\partial F}{\partial \mathbf{c}_t } \;\; \right]^\intercal_{T \times T }\] <ul> <li><strong>Backward step 4:</strong> because of $\mathbf{q}_i = \mathbf{\mathbf{A}} \mathbf{x}_i$, we have</li> </ul> \[\frac{\partial F}{\partial \mathbf{A}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{q}_i} \frac{\partial \mathbf{q}_i}{\partial \mathbf{A}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{q}_i} \mathbf{x}_i^\intercal = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] <p>Putting all the above steps together, we have</p> \[\frac{\partial F}{\partial \mathbf{A}} = \frac{1}{\sqrt{h}}\bigg[ \;\; \mathbf{K} \;\; \bigg]_{h \times T} \Bigg( \mathcal{A} \otimes \Bigg( \bigg[ \;\; \mathbf{V}^\intercal \;\; \bigg]_{T\times h } \bigg[ \;\; \mathbf{E} \;\; \bigg]_{h \times T } \Bigg) \Bigg)^\intercal \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] \[= \frac{1}{\sqrt{h}} \mathbf{K} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg)^\intercal \mathbf{X}^\intercal\] <p>Similarly, we can derive</p> \[\frac{\partial F}{\partial \mathbf{B}} = \frac{1}{\sqrt{h}} \mathbf{Q} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg) \mathbf{X}^\intercal\] <p>Since we have</p> \[\mathbf{z}_t = \sum_{i=1}^T a_{it} \mathbf{v}_i\;\; (\forall t = 1,2, \cdots, T)\] <p>We compute</p> \[\frac{\partial F}{ \partial \mathbf{v}_i} = \sum_{t=1}^T a_{it} \frac{\partial F}{\partial \mathbf{z}_t}\] <p>Arrange them column by column into a matrix</p> \[\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\;\bigg]_{h \times T} = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{z}_i} \;\;\bigg]_{h \times T} \bigg[ \;\; a_{it} \;\; \bigg]^\intercal_{T \times T} = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{z}_i} \;\;\bigg]_{h \times T} \mathcal{A}^\intercal = \mathbf{E} \mathcal{A}^\intercal\] <p>Since we have $\mathbf{v}_i = \mathbf{C} \mathbf{x}_i \;\; (\forall i=1,2,\cdots,T)$ , we compute</p> \[\frac{\partial F}{\partial \mathbf{C}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{v}_i} \frac{\partial \mathbf{v}_i}{\partial \mathbf{C}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{v}_i} \mathbf{x}_i^\intercal = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\; \bigg]_{h \times T} \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] <p>As a result, we have</p> \[\frac{\partial F}{\partial \mathbf{C }} = \mathbf{E} \mathcal{A}^\intercal \mathbf{X}^\intercal\] <p>Finally, we back-propagate the error signals from output to input. The input $\mathbf{X}$ affects the output through three different paths, i.e. $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$. Therefore, we have</p> \[\frac{\partial F}{\partial \mathbf{X}} = \frac{\partial F}{\partial \mathbf{Q}} \frac{\partial \mathbf{Q}}{ \partial \mathbf{X}} + \frac{\partial F}{\partial \mathbf{K}} \frac{\partial \mathbf{K}}{ \partial \mathbf{X}} + \frac{\partial F}{\partial \mathbf{V}} \frac{\partial \mathbf{V}}{ \partial \mathbf{X}}\] \[= \bigg[ \;\; \mathbf{A}^\intercal \;\; \bigg]_{d\times h} \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} + \bigg[ \;\; \mathbf{B}^\intercal \;\; \bigg]_{d\times h} \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{k}_i} \;\; \bigg]_{h \times T} + \bigg[ \;\; \mathbf{C}^\intercal \;\; \bigg]_{d\times l}\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\; \bigg]_{h \times T}\] \[= \frac{1}{\sqrt{h}} \bigg( \mathbf{A}^\intercal \mathbf{K} \Big( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \Big)^\intercal + \mathbf{B}^\intercal \mathbf{Q} \Big( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \Big) \bigg) + \mathbf{C}^\intercal \mathbf{E} \mathcal{A}^\intercal\] <p>Finally, we summarize the above results using a more compact matrix representation. If we define the following $3h \times T$ matrix:</p> \[\mathbf{P} \overset{\Delta}{=} \begin{bmatrix} \frac{1}{\sqrt{h}} \mathbf{K} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg)^\intercal \\ \frac{1}{\sqrt{h}} \mathbf{Q} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg) \\ \mathbf{E} \mathcal{A}^\intercal \end{bmatrix}_{ 3h \times T}\] <p>we have</p> \[\begin{bmatrix} \frac{\partial F}{\partial \mathbf{A}} \\[0.1cm] \frac{\partial F}{\partial \mathbf{B}} \\[0.1cm] \frac{\partial F}{\partial \mathbf{C}} \end{bmatrix} = \mathbf{P} \, \mathbf{X}^\intercal \;\;\;\;\;\;\Big(\in \mathbb{R}^{3h \times d} \Big)\] \[\frac{\partial F}{\partial \mathbf{X}} = \begin{bmatrix} \mathbf{A} \\ \mathbf{B} \\ \mathbf{C} \end{bmatrix}^\intercal \, \mathbf{P} = \Big[ \mathbf{A}^\intercal \;\; \mathbf{B}^\intercal \;\; \mathbf{C}^\intercal \Big] \, \mathbf{P} \;\;\;\;\;\;\Big(\in \mathbb{R}^{d \times T} \Big)\] <p><em>Note: Refer to <a href="https://colab.research.google.com/drive/1-op--04cwJI2L8iPNIgNoPsJ3uPke8OB">a pytorch implementation at Colab</a> and its comparison with pytorch autograd (or a <a href="https://colab.research.google.com/drive/1WOdo0AuSn-lIzOxXLxTwDAKPJhzEvJ2e">JAX implementation at Colab</a>).</em></p> <h2 id="gpt-3"><strong>GPT-3</strong></h2> <p>The authors of <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite> propose a deep multi-head transformer structure named <em>GPT-3</em> for processing text data. In GPT-3, the values of $d$, $h$, and $T$ are set to $d=12288$, $h=128$, and $T=2048$, respectively, while the vocabulary is composed of $50257$ distinct tokens.</p> <p>To begin, <em>GPT-3</em> employs a tokenizer to split any text sequence into a sequence of tokens (each token is a common word fragment). These tokens are then transformed into vectors of $d=12288$ dimensions using a word embedding matrix $\mathbf{W}_0 \in \mathbb{R}^{12288 \times 50257}$. Subsequently, the input sequence $\mathbf{X} \in \mathbb{R}^{12288 \times 2048}$ is transformed into $\mathbf{Y} \in \mathbb{R}^{12288 \times 2048}$ using 96 layers of multi-head transformer blocks. Each block is defined as follows:</p> <p><strong>(1)</strong> Each multi-head transformer in <em>GPT-3</em> uses $96$ heads:</p> \[\mathbf{A}^{(j)}, \mathbf{B}^{(j)}, \mathbf{C}^{(j)} \in \mathbb{R}^{128 \times 12288} \;\;\; (j=1,2, \cdots, 96)\] <p>which compute for all $j=1,2,\cdots,T$:</p> \[\mathbf{Z}^{(j)} \in \mathbb{R}^{ 128 \times 2048} = \big( \mathbf{C}^{(j)} \mathbf{X} \big) \; \textrm{softmax}\Big( \big(\mathbf{A}^{(j)} \mathbf{X} \big)^\intercal \big( \mathbf{B}^{(j)} \mathbf{X} \big)/\sqrt{128}\Big)\] <p><strong>(2)</strong> Concatenate the outputs from all heads:</p> \[\mathbf{Z} \in \mathbb{R}^{12288 \times 2048} = \mathbf{W}^o \textrm{concat}\big(\mathbf{Z}^{(1)}, \cdots, \mathbf{Z}^{(96)}\big)\] <p>where $\mathbf{W}^o \in \mathbb{R}^{12288\times12288}$.</p> <p><strong>(3)</strong> Apply layer normalization to each column of $\mathbf{Z}$: $\mathbf{z}_t \in \mathbb{R}^{12288} \; (\forall t=1,2,\cdots,2048)$ as</p> \[\bar{\mathbf{z}}_t = \mathbf{x}_t + \textrm{LN}_{\gamma,\beta} \big(\mathbf{z}_t \big)\] <p><strong>(4)</strong> Apply nonlinearity to each column as:</p> \[\mathbf{y}_t= \bar{\mathbf{z}}_t + \textrm{feedforward} \big( \bar{\mathbf{z}}_t\big) = \bar{\mathbf{z}}_t + \mathbf{W}_2 \textrm{ReLU} (\mathbf{W_1} \bar{\mathbf{z}}_t)\] <p>where $\mathbf{W}_1 \in \mathbb{R}^{49152 \times 12288}$, and $\mathbf{W}_2 \in \mathbb{R}^{12288 \times 49152}$.</p> <p>Based on these, we may calculate that the total number of parameters in <em>GTP-3</em> is about $175$ billions.</p> <p>During training, a sequence of training vectors</p> \[\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_{2048} \}\] <p>is fed into <em>GPT-3</em> as input $\mathbf{X} \in \mathbb{R}^{12288\times 2048}$. For each time step $t=1,2,\cdots,2047$, <em>GPT-3</em> is trained to predict the token at position $t+1$ based on all vectors appearing up to position $t$, i.e., ${\mathbf{x}_1, \cdots, \mathbf{x}_t}$.</p> <p>After its training, GPT-3 has the ability to create new sequences by using an input sequence as a prompt. To do this, the model calculates the probabilities of the possible next tokens that could follow the given prompt, and then selects a new token by randomly sampling from these probabilities. The selected token is then added to the end of the prompt, forming a new prompt. This process continues until the model generates a termination token.</p>]]></content><author><name>Hui Jiang</name></author><category term="machine-learming"/><category term="AI"/><category term="NLP"/><summary type="html"><![CDATA[In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture.]]></summary></entry><entry><title type="html">Machine Learning Fundamentals</title><link href="https://incml.github.io/huijiang/blog/2022/Machine-Learning-Fundamentals/" rel="alternate" type="text/html" title="Machine Learning Fundamentals"/><published>2022-01-21T01:00:00+00:00</published><updated>2022-01-21T01:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2022/Machine-Learning-Fundamentals</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2022/Machine-Learning-Fundamentals/"><![CDATA[<div class="row"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/mlb-book-cover-small-480.webp 480w,/huijiang/assets/img/mlb-book-cover-small-800.webp 800w,/huijiang/assets/img/mlb-book-cover-small-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/mlb-book-cover-small.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This post provides complementary materials for my recent textbook <a href="https://www.amazon.com/Machine-Learning-Fundamentals-Concise-Introduction/dp/1108940021"><strong>Machine Learning Fundamentals</strong></a> by <a href="https://incml.github.io/huijiang/"><em>Hui Jiang</em></a>, Cambridge University Press, 2021.</p> <h3 id="slides-per-chapter-detailed--contents-is-here">Slides per chapter (Detailed contents is <a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:detailedcontents.pdf">here</a>)</h3> <ul> <li><strong>Ch 1: Introduction</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch1_introduction.pdf">slides</a>)</li> <li><strong>Ch 2: Mathematical Foundation</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch2_mathematical_foundation.pdf">slides</a>)</li> <li><strong>Ch 3: Supervised Machine Learning (in a nutshell)</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch3_supervised_machine_learning.pdf">slides</a>)</li> <li><strong>Ch 4: Feature Extraction</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch4_feature_extraction.pdf">slides</a>)</li> <li><strong>Ch 5: Statistical Learning Theory</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch5_statistical_learning_theory.pdf">slides</a>)</li> <li><strong>Ch 6: Linear Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch6_linear_models.pdf">slides</a>)</li> <li><strong>Ch 7: Learning Discriminative Models in General</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch7_learning_discriminative_models.pdf">slides</a>)</li> <li><strong>Ch 8: Neural Networks</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch8_neural_networks.pdf">slides</a>)</li> <li><strong>Ch 9: Ensemble Learning</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch9_ensemble_learning.pdf">slides</a>)</li> <li><strong>Ch 10: Overview of Generative Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch10_overview_generative_models.pdf">slides</a>)</li> <li><strong>Ch 11: Unimodal Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch11_unimodal_models.pdf">slides</a>)</li> <li><strong>Ch 12: Mixture Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch12_mixture_models.pdf">slides</a>)</li> <li><strong>Ch 13: Entangled Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch13_entangled_models.pdf">slides</a>)</li> <li><strong>Ch 14: Bayesian Learning</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch14_bayesian_learning.pdf">slides</a>)</li> <li><strong>Ch 15: Graphical Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch15_graphical_models.pdf">slides</a>)</li> </ul> <h3 id="lab-projects-using-jupyter-notebooks">Lab Projects (using Jupyter Notebooks)</h3> <ul> <li><strong>Lab 0</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab0_Preparation.pdf">Preparation</a> (available at <a href="https://colab.research.google.com/drive/1nB-uULhbpFTqlUOekEW3272eqKJg6qok?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab0_Preparation.ipynb">ipynb</a>)</li> <li><strong>Lab 1</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab1_Data_Visualization.pdf">Data Visualization</a> (available at <a href="https://colab.research.google.com/drive/1zE2OrdJNkmzzn30CdDsj1y9qz585-afF?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab1_Data_Visualization.ipynb">ipynb</a>)</li> <li><strong>Lab 2</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab2_Linear_Regression.pdf">Linear Regression</a> (available at <a href="https://colab.research.google.com/drive/1Ix04T8y0evnFnXWvHHWu0KfqsOA48hUg?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab2_Linear_Regression.ipynb">ipynb</a>)</li> <li><strong>Lab 3</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab3_Logistic_Regression.pdf">Logistic Regression</a> (available at <a href="https://colab.research.google.com/drive/1qUi2qgwBMmHJb7Bx7ca_braGtptmzFOQ?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab3_Logistic_Regression.ipynb">ipynb</a>)</li> <li><strong>Lab 4</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab4_Support_Vector_Machine.pdf">Support Vector Machines</a> (available at <a href="https://colab.research.google.com/drive/1vlN46Xhfv4ES8jFWWI0iTj3WHKjb0KMg?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab4_Support_Vector_Machine.ipynb">ipynb</a>)</li> <li><strong>Lab 5</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab5_Fully_Connected_Neural_Networks.pdf">Fully-Connected Neural Networks</a> (available at <a href="https://colab.research.google.com/drive/1TY3fAAS18lyGrlEsvLhaYm6fIBP7CDgD?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab5_Fully_Connected_Neural_Networks.ipynb">ipynb</a>)</li> <li><strong>Lab 6</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab6_Convolutional_Neural_Networks.pdf">Convolutional Neural Networks</a> (available at <a href="https://colab.research.google.com/drive/1Zkeujeoh4jNR9bQ67kfuIE7cV6KifZS2?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab6_Convolutional_Neural_Networks.ipynb">ipynb</a>)</li> <li><strong>Lab 7</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab7_Transformers.pdf">Transformers</a> (available at <a href="https://colab.research.google.com/drive/1RvaBdX-KrgCNkVoo39UYqH1qzd-hAT5q">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab7_Transformers.ipynb">ipynb</a>)</li> <li><strong>Lab 8</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab8_Matrix_Factorization.pdf">Matrix Factorization</a> (available at <a href="https://colab.research.google.com/drive/1JaGmlkrC03URmnfkKtOakckP_ds20-WA">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab8_Matrix_Factorization.ipynb">ipynb</a>)</li> <li><strong>Lab 9</strong>: Decision Trees, Random Forests and Boosted Trees</li> <li><strong>Lab 10</strong>: Gaussian Classifiers and Gaussian Mixture Models</li> </ul> <h3 id="citation-bibtex">Citation (bibtex):</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@book{Jiang-MLF-2021, 
  author = {Hui Jiang},
  title = {Machine Learning Fundamentals}, 
  publisher = {Cambridge University Press},
  year = {2021} 
}
</code></pre></div></div>]]></content><author><name></name></author><category term="machine-learming"/><category term="AI"/><summary type="html"><![CDATA[Supplementary materials for the book "Machine Learning Fundamentals", published by Cambridge University Press.]]></summary></entry></feed>