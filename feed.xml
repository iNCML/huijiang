<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://incml.github.io/huijiang/feed.xml" rel="self" type="application/atom+xml"/><link href="https://incml.github.io/huijiang/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-15T14:06:41+00:00</updated><id>https://incml.github.io/huijiang/feed.xml</id><title type="html">blank</title><subtitle>Prof. Hui Jiang&apos;s personal website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Brief Introduction to Bitcoin</title><link href="https://incml.github.io/huijiang/blog/2025/Bitcoin/" rel="alternate" type="text/html" title="A Brief Introduction to Bitcoin"/><published>2025-08-14T00:00:00+00:00</published><updated>2025-08-14T00:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2025/Bitcoin</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2025/Bitcoin/"><![CDATA[<p>In this post, we provide a concise introduction to the key technologies that underpin the Bitcoin network. This overview highlights several important implementations within Bitcoin and offers readers the foundational knowledge needed to understand how the network operates. More specifically, we cover how Bitcoin addresses are created, how wallets work, how the network generates and verifies transactions, how proof-of-work powers the blockchain, and new features like SegWit. Interested readers can refer to <d-cite key="Antonopoulos2014"></d-cite> <d-cite key="Rosenbaum2019"></d-cite><d-cite key="Ammous2018"></d-cite> for more technical details.</p> <h2 id="bitcoin-address"><strong>Bitcoin Address</strong></h2> <p>In the Bitcoin network, any bitcoin funds are essentially associated with a bitcoin address, which has a corresponding secret password. Bitcoin addresses, much like email addresses, are public information and can be viewed by anyone on the Bitcoin network. This means that anyone can track all transactions associated with a given address or send funds to it. However, in order to spend or transfer the bitcoins held at an address, the owner must use the secret password. Without this secret password, which must be kept secure and never shared, it is impossible to access or move the funds. From the perspective of cryptography, the bitcoin address and its corresponding password are equivalent to a pair of public and private keys. More precisely, we first draw a large random number (either 128-bit or 256-bit) as a private key. Using cryptographic techniques – most commonly elliptic curve cryptography (ECC) – a corresponding public key is then derived from this private key. As is well established, it is computationally infeasible to determine the private key from the public key. The diagram below illustrates the steps by which the Bitcoin network converts a public key into a Bitcoin address used on the network.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/key-to-bitcoin-address-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/key-to-bitcoin-address-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/key-to-bitcoin-address-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/key-to-bitcoin-address.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. A diagram to show how to compute the bitcoin address based on a public key (Image adapted from <d-cite key="Antonopoulos2014"></d-cite>). </div> <p>In Figure 1, there are three key components that are explained in detail below:</p> <ol> <li> <p><strong>SHA256</strong>: a hash function that generates a 256-bit hush from any inputs.</p> </li> <li> <p><strong>RIPEMD-160</strong>: another hash function generates a 160-bit hash from any input. The use of double hashing – first applying SHA-256 and then RIPEMD-160 – enhances the security of the resulting hash by reducing the risk of collisions and preventing reversal from the hash back to the original input. At the same time, it shortens the length of a Bitcoin address from 256 bits to 160 bits, thereby greatly saving space in the shared Bitcoin ledger.</p> </li> <li> <p><strong>Base58check</strong>: this module serves two purposes: i) It applies an encoding method known as Base58 to represent data, which improves upon the commonly used Base64 encoding scheme. Unlike Base64, which relies on 64 characters, Base58 uses only 58 unambiguous characters, omitting the six confusing ones (0, O, l, I, +, /) to make it more human-friendly. ii) To provide additional protection against typos and transcription errors, a checksum mechanism is included. The checksum is generated by taking the first four bytes of the double SHA-256 hash of the input data. These bytes are then appended to the input data before applying Base58 encoding. See the following diagram for all steps in Base58check.</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/base58check-encoding-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/base58check-encoding-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/base58check-encoding-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/base58check-encoding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The following diagrams further explain the encoding and decoding processes between public key and its corresponding Bitcoin address:</p> <ol> <li>Convert a public key ‘5f26….1e60’ into a Bitcoin address ‘19g6….RBPD’</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/key-to-address-example-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/key-to-address-example-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/key-to-address-example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/key-to-address-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>Convert a bitcoin address back to its corresponding public key:</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/address-back-key-example-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/address-back-key-example-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/address-back-key-example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/address-back-key-example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="wallets"><strong>Wallets</strong></h2> <p>If a Bitcoin user relies on the same address for all transactions, this can create security and privacy risks:</p> <ul> <li> <p>Since the private key is required to sign every transaction from that address, repeated use increases the likelihood of the private key being exposed.</p> </li> <li> <p>Because all addresses and transaction details are publicly visible on the network, consistently using a single address makes it easy to link all transactions to one user. Even if the user’s real-world identity is not directly revealed, this practice still represents a significant violation of privacy.</p> </li> </ul> <p>The solution to these problems is to use each Bitcoin address only once, requiring users to generate a new address for every transaction. As a result, a user may need to manage a large number of addresses, each with its own private key. To simplify this process, the concept of a wallet was introduced. A wallet allows a user to store and manage multiple Bitcoin addresses along with their corresponding private keys, all under the control of that user.</p> <p>In addition, a wallet can provide several other important functions for users. For instance, it can help initiate transactions from its stored addresses, using the securely held private keys to sign these transactions before broadcasting them to the network. A wallet can also continuously monitor the Bitcoin network to track the funds associated with all of its addresses. Moreover, it should offer reliable backup options or a secure recovery mechanism, enabling users to restore their private keys and addresses in case the wallet is lost or damaged.</p> <p>If all addresses and private keys in a wallet were generated independently at random, backing them up or restoring them would be extremely cumbersome. Hierarchical Deterministic (HD) wallets solve this problem by using a single random number, known as a seed, to generate as many private key–address pairs as needed. This approach allows the entire wallet to be backed up simply by recording the seed. If necessary, all addresses and private keys can be fully restored by applying the same generation method to that seed.</p> <h3 id="hierarchical-deterministic-hd-wallets"><strong>Hierarchical Deterministic (HD) Wallets</strong></h3> <p>The key idea behind HD wallets is to extend each private key into an extended private key (xprv), which consists of two components: the private key itself and a chain code. Likewise, each public key can be extended into an extended public key (xpub), containing the public key and a corresponding chain code. These chain codes are derived by applying HMAC-SHA512 to input data, producing a 512-bit hash that is then split into two parts: a 256-bit chain code and a 256-bit value used as the private key in an xprv or as the public key in an xpub. The chain codes enable the generation of additional xprvs or xpubs in a hierarchical tree structure, following a specified derivation path.</p> <ol> <li><strong>Deriving a master xprv and xpub</strong>: The wallet seed is processed with HMAC-SHA512 to produce a 512-bit hash. This hash is then split into two halves: the left 256 bits form the master private key, while the right 256 bits serve as the chain code. The chain code is subsequently used to derive additional private keys in the next steps.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-xpub-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-xpub-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-xpub-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-xpub.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li><strong>Deriving hardened private keys for all child nodes</strong>: Each node in the tree is identified by a derivation path. For example, the derivation path ‘m/0/1’ indicates the second grandchild of the first child of the root, where m denotes the master private key. Child private keys are derived recursively in a top-down manner according to their respective derivation paths. The diagram below illustrates this process, showing how the ‘hardened’ private key m/1’ can be derived from its parent key m. Note that the corresponding public key for this node, denoted as M/1’, can be derived from the private key m/1’ using the standard cryptographic technique.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-child-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-child-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-child-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-child.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>The whole process is shown as below:</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-1-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-1-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Alternatively, all child xpubs can be derived directly from their parent xpubs, without accessing the corresponding private keys (xprvs). Since this process never exposes private key information, it can safely be performed on a less secure server, such as a web server.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-2-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-2-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/xprv-whole-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It is important to note that cryptographic analysis shows this alternative procedure—deriving a child extended public key (xpub) directly by adding it to the parent public key—is mathematically equivalent to the first method, in which the child extended private key (xprv) is first computed by adding it to the parent private key and then converted into the corresponding xpub. This alternative method is preferable because it does not expose the parent private keys.</p> <h3 id="mnemonic-sentences"><strong>Mnemonic sentences</strong></h3> <p>In an HD wallet, the long random seed can be encoded into a sequence of human-readable mnemonic words. Recording 12 or 24 simple words is far more convenient and user-friendly than writing down a lengthy and complex hexadecimal seed.</p> <ol> <li><strong>Encoding a seed into a mnemonic sentence</strong>: the following diagram shows an example to encode a 128-bit random seed into a mnemonic sentence of 12 English words.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/seed-to-mnem-words-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/seed-to-mnem-words-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/seed-to-mnem-words-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/seed-to-mnem-words.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li><strong>Decoding a mnemonic sentence into a seed</strong>: The following diagram shows the inverse process to convert a mnemonic sentence back to the seed. Here, the 4-bit checksum provides only a limited safeguard against typos and transcription errors when recording mnemonic words.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/mnem-words-to-seed-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/mnem-words-to-seed-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/mnem-words-to-seed-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/mnem-words-to-seed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="transactions"><strong>Transactions</strong></h2> <p>On the Bitcoin network, users create transactions to transfer funds from one or more addresses to other addresses. Each transaction specifies the funds being spent (the inputs) and the destinations for those funds (the outputs). An input references a previous transaction by including its transaction ID (txid) – a double SHA-256 hash of the transaction – along with an index that identifies which output of the previous transaction is being used. An output specifies both the amount of bitcoin being sent and the recipient’s Bitcoin address where the funds will be stored.</p> <p>When a transaction is created, each input must be signed with the private key corresponding to the address, proving that the user legitimately controls the funds. The public key and the generated signature are then included in the transaction so that every node on the network can verify it. The public key is necessary because Bitcoin addresses are derived from public keys through hashing, and the hash function prevents reconstructing the public key directly from the address. Each signature is bound to the entire transaction, meaning the signing algorithm hashes all transaction data (excluding the signatures themselves). Consequently, any modification to the transaction after signing will invalidate the signature, making tampering immediately detectable.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/transactions-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/transactions-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/transactions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/transactions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To verify a transaction, two checks must be performed: (i) whether the provided public key corresponds to the source address, and (ii) whether the signature is valid. This verification process is carried out using small scripts, essentially miniature programs. The first component, called the signature script (or scriptSig), contains the signature and the public key. The second component, called the public key script (or scriptPubKey), contains the Bitcoin address – specifically the public key hash (PKH) – along with the instructions for how the verification should be executed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/verify-transaction-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/verify-transaction-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/verify-transaction-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/verify-transaction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Both parts are combined to execute on a stack-based virtual machine to verify the transaction.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/stack-vm-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/stack-vm-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/stack-vm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/stack-vm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To create new bitcoin, a special type of transaction called a <em>coinbase</em> transaction is used. Unlike regular transactions, it has no source addresses. Ultimately, all bitcoins in circulation can be traced back to one or more coinbase transactions.</p> <h2 id="the-blockchain"><strong>The Blockchain</strong></h2> <p>All new transactions on the network are grouped into a block roughly every 10 minutes. This block is then added to the chain of previous blocks, known as the blockchain. Each block contains cryptographic hash links that ensure the integrity of the entire chain, making the blockchain immutable.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/blockchain-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/blockchain-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/blockchain-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/blockchain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Each block consists of a header and all the transactions included within it.</p> <p>A block header contains:</p> <ol> <li>The hash of the previous block’s header,</li> <li><strong>The Merkle root</strong>, representing the combined hash of all transactions in the block,</li> <li>A timestamp indicating when the block was created, and</li> <li>A hash value demonstrating <strong>proof-of-work</strong>.</li> </ol> <h3 id="the-merkle-tree"><strong>The Merkle Tree</strong>:</h3> <p>The Merkle root is the combined hash of all transactions in a block. It is calculated by creating a hierarchy of cryptographic hashes, namely a Merkle tree.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/merkle-tree-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/merkle-tree-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/merkle-tree-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/merkle-tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>From the above, it is clear that modifying any transaction within a block will alter the Merkle root, which is sufficient to validate the integrity of all transactions in that block. Another important use of the Merkle tree is that lightweight nodes can verify whether a specific transaction is included in a block without downloading the entire block. They can do this using only the block header together with a partial Merkle tree that contains the hashes along the path from the transaction up to the root in the Merkle tree.</p> <h3 id="proof-of-work"><strong>Proof of Work</strong></h3> <p>Miners on the network compete to create the next block to be added to the blockchain. The first miner to produce a valid proof-of-work earns the right to add its block to the chain. To generate a valid proof-of-work, the miner must find a block header hash (by varying the value of the nonce) that is less than or equal to the difficulty target specified by the network. The difficulty target is dynamically adjusted by the network to ensure that one new block is added to the blockchain in roughly 10 minutes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/proof-of-work-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/proof-of-work-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/proof-of-work-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/proof-of-work.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="segregated-witness"><strong>Segregated Witness</strong></h2> <p>Segregated Witness (SegWit) was a major technical upgrade to the Bitcoin protocol introduced in 2015. Its core idea is to separate signature data from transactions and place it into an external structure called the witness. As a result, a transaction’s ID (txid) is no longer tied to its signature data.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2025-08-14-Bitcoin/segwit-480.webp 480w,/huijiang/assets/img/2025-08-14-Bitcoin/segwit-800.webp 800w,/huijiang/assets/img/2025-08-14-Bitcoin/segwit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2025-08-14-Bitcoin/segwit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>SegWit brought several important improvements to Bitcoin. Most notably, it solved the transaction malleability problem, where a txid could be altered by modifying the signature portion of a transaction. In addition, SegWit allows more transactions to fit into each block, since individual transactions become smaller. It also reduces network bandwidth usage, as signature data does not always need to be transmitted across the network.</p> <p>SegWit was carefully designed to ensure both forward and backward compatibility:</p> <ol> <li> <p><strong>Forward compatibility</strong> – Nodes running older software can still validate SegWit transactions without rejecting them.</p> </li> <li> <p><strong>Backward compatibility</strong> – Transactions created by older software remain valid and function correctly with updated programs.</p> </li> </ol> <p>SegWit introduces a new address format encoded with <strong>Bech32</strong> instead of Base58Check, for example: <em>bc1qeqzjk7vume5wmrdgz5xyehh54cchdjag6jdmkj</em>. These SegWit addresses can be converted into what is known as a witness program, which is used during transaction verification to ensure compatibility so that both old and new software implementations can process them correctly. Furthermore, SegWit transactions can be included in the same block alongside legacy transactions. If a block contains any SegWit transactions, an additional Merkle tree is constructed from all witness data to produce a combined hash, known as the witness commitment. This witness commitment is stored in the coinbase transaction, allowing updated SegWit-enabled nodes to properly verify blocks that include SegWit transactions.</p>]]></content><author><name>Hui Jiang</name></author><category term="bitcoin"/><category term="crypto"/><summary type="html"><![CDATA[In this post, we provide a concise introduction to the key technologies that underpin the Bitcoin network. This overview highlights several important implementations within Bitcoin and offers readers the foundational knowledge needed to understand how the network operates.]]></summary></entry><entry><title type="html">A Deterministic View of Diffusion Models</title><link href="https://incml.github.io/huijiang/blog/2024/Deterministic-Diffusion-Models/" rel="alternate" type="text/html" title="A Deterministic View of Diffusion Models"/><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2024/Deterministic-Diffusion-Models</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2024/Deterministic-Diffusion-Models/"><![CDATA[<p>In recent years, diffusion models, a novel category of deep generative models <d-cite key="JiangMLF2021"></d-cite>, have made significant strides in producing high-quality, high-resolution images. Notable examples include GLIDE <d-cite key="nichol2022glidephotorealisticimagegeneration"></d-cite>, DALLE-2 <d-cite key="ramesh2022hierarchicaltextconditionalimagegeneration"></d-cite>, Imagen <d-cite key="saharia2022photorealistictexttoimagediffusionmodels"></d-cite>, and the fully open-source Stable Diffusion <d-cite key="rombach2022highresolutionimagesynthesislatent"></d-cite>. These models are traditionally developed based on the framework of Denoising Diffusion Probabilistic Models (DDPM) <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite>. In this probabilistic framework, the forward diffusion process is modeled as a Gaussian process with Markovian properties. Conversely, the backward denoising process employs neural networks to estimate the conditional distribution at each time step. The neural networks involved in the denoising process are trained to minimize the evidence lower bound (ELBO) <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite>, akin to the approach used in a Variational Autoencoder (VAE) <d-cite key="Kingma_2019"></d-cite>.</p> <p>In this post, we present a deterministic perspective on diffusion models. In this method, neural networks are constructed to function in the opposite way of a deterministic diffusion process that gradually deteriorates images over time. This training allows the neural networks to reconstruct or generate images by reversing the diffusion process without using any knowledge of stochastic process. This method simplifies the derivation of diffusion models, making the process more straightforward and comprehensible. Within this deterministic framework, diffusion models can be fully explained and derived from scratch using only a few straightforward mathematical equations, as shown in this concise, self-contained post. This approach requires only basic mathematical knowledge, eliminating the need for lengthy tutorials filled with hundreds of complex equations involving stochastic processes and probability distributions. <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite>.</p> <h2 id="deterministic-forward-diffusion-process"><strong>Deterministic Forward Diffusion Process</strong></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. A deterministic view of diffusion models in the forward diffusion process and the backward denoising transformation (Image adapted from <d-cite key="karagiannakos2022diffusionmodels"></d-cite>). </div> <p>In Figure 1, we illustrate a deterministic view of the typical diffusion process in diffusion models. Starting with any input clean image, denoted as \(\mathbf{x}_0\), the forward process incrementally corrupts the input image for each time step \(t=1,2,\cdots,T\). This corruption is achieved by progressively adding varying levels of Gaussian noises over time as</p> \[\mathbf{x}_t = \sqrt{\alpha_t } \mathbf{x}_{t-1} + \sqrt{1- \alpha_t } \, {\boldsymbol \epsilon}_t \;\;\; \forall t=1, 2, \cdots, T\] <p>adhering to a predefined noise schedule: \(\alpha_1, \alpha_2, \ldots, \alpha_T\), where the noise at each timestep is Gaussian, \({\boldsymbol \epsilon}_t \sim \mathcal{N}(0, \mathbf{I})\). This process gradually introduces more noise at each step, leading to a sequence of increasingly corrupted versions of the original image: \(\mathbf{x}_0 \to \mathbf{x}_1 \to \mathbf{x}_2 \to \cdots \to \mathbf{x}_T\). When \(T\) is large enough, the last image \(\mathbf{x}_T\) approaches to a Gaussian noise, i.e. \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\).</p> <p>Building on the so-called “nice property” outlined in <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite> <d-cite key="karagiannakos2022diffusionmodels"></d-cite>, the above diffusion process can be implemented much more efficiently. Rather than sampling a unique Gaussian noise at each time step, it is feasible to sample a single Gaussian noise, \(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\), and employ the subsequent formula to efficiently generate all the corrupted samples in one go (along the left-to-right red dash arrow in Figure 1):</p> \[\begin{align} \mathbf{x}_t = f(\mathbf{x}_{0}, {\boldsymbol \epsilon}, t) = \sqrt{\bar{\alpha}_t} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \, {\boldsymbol \epsilon} \;\;\; \forall t=1, 2, \cdots, T \label{eq-forward-deterministic} \end{align}\] <p>where \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\) and we have \(\bar{\alpha}_t \to 0\) as \(t \to T\).</p> <p>In the diffusion process described in Eq. (\ref{eq-forward-deterministic}), once the noise \({\boldsymbol \epsilon}\) is sampled, it is treated as constant for the whole diffusion process. Consequently, the transformation from the clean image \(\mathbf{x}_{0}\) to noisy images \(\mathbf{x}_{t}\) at each time step \(t\) can be regarded as a deterministic mapping, characterized by the above function \(\mathbf{x}_t = f(\mathbf{x}_{0}, {\boldsymbol \epsilon}, t)\).</p> <p>As shown in Figure 2, clean images are gradually converted into pure noises in the above deterministic diffusion process as \(t\) goes from \(0\) to \(T\). The method in Eq.(\ref{eq-forward-deterministic}) streamlines the process, making the generation of corrupted samples more straightforward and less computationally demanding.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic-diffusion-process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2. The deterministic diffussion process of some images selected from the MNIST-Fashion dataset. </div> <h2 id="deterministic-backward-denoising-process"><strong>Deterministic Backward Denoising Process</strong></h2> <p>If the forward diffusion process is treated as deterministic, the corresponding deterministic mappings for the backward denoising process – transforming any noisy image \(\mathbf{x}_{t}\) back to the clean image \(\mathbf{x}_{0}\) – can also be derived.</p> <p>Here, let’s explore the relationship between \(\mathbf{x}_{t-1}\) and \(\mathbf{x}_t\) in the above diffusion process. This exploration will help us understand how consecutive stages in the diffusion process are linked by a deterministic function, which is essential for the subsequent deterministic denoising methods. In fact, it is possible to establish this deterministic function connecting two consecutive samples using two different approaches.</p> <p><strong>(1)</strong> In the first method, assuming the noise \({\boldsymbol \epsilon}\) is known, we can rearrange eq.(\ref{eq-forward-deterministic}) for the time step, \(t\), as follows:</p> \[\begin{align} \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \big[ \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, {\boldsymbol \epsilon} \big] \label{eq-forwar-t} \end{align}\] <p>Furthermore, according to Eq.(\ref{eq-forward-deterministic}), for the previous time step \(t-1\), we have the following:</p> \[\begin{align} \mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \label{eq-forwar-t-1} \end{align}\] <p>We may substitue Eq.(\ref{eq-forwar-t}) into the above equation to derive the first relationship between any two adjacent samples as follows:</p> \[\begin{align} \begin{aligned} \mathbf{x}_{t-1} &amp;= \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \\ &amp;= \frac{1}{\sqrt{\alpha_t}} \big[ \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, {\boldsymbol \epsilon} \big] + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \\ &amp;= \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) {\boldsymbol \epsilon} \Big] \end{aligned} \label{eq-consecutive-time-noise} \end{align}\] <p><strong>(2)</strong> Alternatively, assuming the original clean image \(\mathbf{x}_{0}\) is known, we can rearrange Eq.(\ref{eq-forward-deterministic}) as follows</p> \[{\boldsymbol \epsilon} = \frac{1}{\sqrt{1 - \bar{\alpha}_{t}}} \big[ \mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}\big]\] <p>and substitute \({\boldsymbol \epsilon}\) into Eq.(\ref{eq-forwar-t-1}), we have</p> \[\begin{aligned} \mathbf{x}_{t-1} &amp;= \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t-1}} \, {\boldsymbol \epsilon} \\ &amp;= \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0} + \frac{\sqrt{1 - \bar{\alpha}_{t-1}}}{\sqrt{1 - \bar{\alpha}_{t}}} \big[ \mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}\big] \\ \end{aligned}\] <p>If we denote</p> \[\bar{\gamma}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\] <p>we can simplify the above equation as follows:</p> \[\begin{align} \mathbf{x}_{t-1} = \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) \mathbf{x}_0 \label{eq-consecutive-time-x0} \end{align}\] <p>However, in practice, we cannot directly use Eq. (\ref{eq-consecutive-time-noise}) or Eq. (\ref{eq-consecutive-time-x0}) to derive \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_t\) during the backward denoising process, as neither the noise \({\boldsymbol \epsilon}\) nor the original clean image ​\(\mathbf{x}_0\) is known. The key insight of diffusion models is that neural networks can be trained to approximate the inverse of the deterministic mapping function \(\mathbf{x}_t = f(\mathbf{x}_{0},\boldsymbol \epsilon, t)\) in the forward process. By leveraging this inverse mapping, the learned neural networks can help to estimate either the noise \({\boldsymbol \epsilon}\) or the original clean image ​\(\mathbf{x}_0\) from a noisy image \(\mathbf{x}_t\). There are two approaches for training neural networks, denoted as \({\boldsymbol \theta}\), to approximate this inverse function:</p> <ol> <li> <p>Approximate the inverse mapping from \(\mathbf{x}_t\) to the clean image \(\mathbf{x}_0\), i.e. \(\mathbf{\hat x}_0 = f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\), allowing Eq. (\ref{eq-consecutive-time-x0}) to be applied for the backward denoising process. This inverse mapping is shown as the right-to-left red dash arrow in Figure 1.</p> </li> <li> <p>Approximate the inverse mapping from \(\mathbf{x}_t\) to the noise \({\boldsymbol \epsilon}\), i.e. \(\hat{\boldsymbol \epsilon} = g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\), enabling the use of Eq. (\ref{eq-consecutive-time-noise}) for the backward denoising process.</p> </li> </ol> <p>Next, we will briefly explore these two different types of deterministic diffusion models.</p> <h2 id="the-deterministic-diffusion-models"><strong>The Deterministic Diffusion Models</strong></h2> <p>In the backward process, starting from a Gaussian noise</p> \[\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\] <p>we may gradually recover all corrupted images backwards one by one until we obtain the initial clean image:</p> \[\mathbf{x}_T \to \mathbf{x}_{T-1} \to \mathbf{x}_{T-2} \to \cdots \to \mathbf{x}_1 \to \mathbf{x}_0\] <p>Alternatively, at any time step \(t\), given the corrupted image \(\mathbf{x}_t\), we may also directly estimate the original clean image \(\mathbf{x}_0\) based on \(\mathbf{x}_t\). If the estimate is good enough, we can terminate the backward denoising process at an earlier stage; Otherwise, we further denoise one time step backwards, i.e. deriving \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_t\). Based on \(\mathbf{x}_{t-1}\), we may derive a better estimate of the clean image \(\mathbf{x}_0\). This denoising process continues until we finally obtain a sufficiently good clean image \(\mathbf{x}_0\).</p> <p>In the above backward denoising process, to obtain a slightly cleaner image \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_{t}\) using either Eq. (\ref{eq-consecutive-time-x0}) or Eq. (\ref{eq-consecutive-time-noise}), we have two options for training neural networks to approximate the inverse mapping, as previously discussed.</p> <h3 id="i-estimating-clean-image-mathbfx_0"><strong>I. Estimating clean image \(\mathbf{x}_0\)</strong></h3> <p>In this case, we construct a deep neural network \(\boldsymbol \theta\) to approximate the inverse function of the above diffusion mapping \(\mathbf{x}_t = f(\mathbf{x}_0, {\boldsymbol \epsilon}, t)\), denoted as</p> \[\mathbf{\hat x}_0 = f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <p>which can recover a rough estimate of the clean image \(\hat{\mathbf{x}}_0\) from any \(\mathbf{x}_t\) (along the right-to-left red dash arrow in Figure 1). In this case, the neural network is learned by minimizing the following objective function over all training data in the training set \(\mathcal{D}\):</p> \[\begin{aligned} L_1({\boldsymbol \theta}) &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) - \mathbf{x}_0\Big)^2 \\ &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( f^{-1}_{\boldsymbol \theta} \big(\sqrt{\bar{\alpha}_t} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \, {\boldsymbol \epsilon}, t \big) - \mathbf{x}_0\Big)^2 \end{aligned}\] <p>Once the neural network \(\boldsymbol \theta\) has been trained, Eq. (\ref{eq-consecutive-time-x0}) can be used to perform the backward denoising process:</p> \[\begin{aligned} \mathbf{x}_{t-1} &amp;= \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) \hat{\mathbf{x}}_0 \\ &amp;= \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) \\ \end{aligned}\] <p>The corresponding sampling process to generate a new image can be described as follows:</p> <ul> <li>sample a Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\)</li> <li><strong>for</strong> \(t=T, T-1, \cdots, 1\): <ul> <li>use the trained neural network \({\boldsymbol \theta}\) to compute</li> </ul> \[\hat{\mathbf{x}}_0 = f^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <ul> <li><strong>if</strong> \(\hat{\mathbf{x}}_0\) is stable <strong>or</strong> \(t=1\), return \(\hat{\mathbf{x}}_0\)</li> <li><strong>else</strong> denoise one step backward as</li> </ul> </li> </ul> \[\mathbf{x}_{t-1} = \sqrt{\bar{\gamma}_t} \, \mathbf{x}_t + \big( \sqrt{\bar{\alpha}_{t-1}} - \sqrt{\bar{\gamma}_t \bar{\alpha}_t} \big) \hat{\mathbf{x}}_0\] <p>In Figure 3, we have shown some sampling results from the MNIST-Fashion dataset through the above sampling algorithm via building neural networks to estimate clean images.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_cleanimage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. Some sampling results are shown from the MNIST-Fashion dataset via building neural networks to estimate clean images. Every two lines represent one sampling example: the first line displays denoising samples at each timestep while the second line shows the estimated clean image at each timestep. </div> <h3 id="ii-estimating-noise-boldsymbol-epsilon"><strong>II. Estimating noise \({\boldsymbol \epsilon}\)</strong></h3> <p>In this case, we construct a deep neural network \(\boldsymbol \theta\) to approximate the inverse function via estimating the noise \({\boldsymbol \epsilon}\) from a corrupted image \(\mathbf{x}_t\) at each time step \(t\):</p> \[\hat{\boldsymbol \epsilon} = g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <p>This neural network is learned by minimizing the following objective function over all training data:</p> \[\begin{aligned} L_2({\boldsymbol \theta}) &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) - {\boldsymbol \epsilon}\Big)^2 \\ &amp;= \sum_{\mathbf{x}_0 \in \mathcal{D}} \sum_{t=1}^T \Big( g^{-1}_{\boldsymbol \theta} \big(\sqrt{\bar{\alpha}_t} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \, {\boldsymbol \epsilon}, t \big) - {\boldsymbol \epsilon}\Big)^2 \end{aligned}\] <p>Once the neural network is learned, we can use Eq.(\ref{eq-consecutive-time-noise}) to derive an estimate of \(\mathbf{x}_{t-1}\) from \(\mathbf{x}_{t}\) as follows:</p> \[\begin{aligned} \mathbf{x}_{t-1} &amp;= \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) \hat{\boldsymbol \epsilon} \Big] \\ &amp;= \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t) \Big] \end{aligned}\] <p>Similarly, the corresponding sampling process to generate a new image can be described as follows:</p> <ul> <li> <p>sample a Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})\)</p> </li> <li> <p><strong>for</strong> \(t=T, T-1, \cdots, 1\):</p> <ul> <li>use the trained neural network \({\boldsymbol \theta}\) to compute:</li> </ul> \[\hat{\boldsymbol \epsilon} = g^{-1}_{\boldsymbol \theta} (\mathbf{x}_t, t)\] <ul> <li>estimate clean image as in Eq.(\ref{eq-forwar-t}):</li> </ul> \[\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \big[ \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, \hat{\boldsymbol \epsilon} \big]\] <ul> <li> <p><strong>if</strong> \(\hat{\mathbf{x}}_0\) is stable <strong>or</strong> \(t=1\), return \(\hat{\mathbf{x}}_0\)</p> </li> <li> <p><strong>else</strong> denoise one step backward as</p> </li> </ul> \[\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \Big[ \mathbf{x}_t - \big( \sqrt{1-\bar{\alpha}_t} - \sqrt{\alpha_t-\bar{\alpha}_t} \big) \hat{\boldsymbol \epsilon} \Big]\] </li> </ul> <p>In Figure 4, we have shown some sampling results from the MNIST-Fashion dataset via building neural networks to estimate noises through the above sampling algorithm.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise-480.webp 480w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise-800.webp 800w,/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2024-11-11-Deterministic-Diffusion-Models/deterministic_denoising_via_noise.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4. Some sampling results are shown from the MNIST-Fashion dataset via building neural networks to estimate noises. Every two lines represent one sampling example: the first line displays denoising samples at each timestep while the second line shows the estimated clean image at each timestep. </div> <h2 id="final-remarks"><strong>Final Remarks</strong></h2> <p>In recent years, diffusion models have become increasingly significant in computer vision, emerging as the dominant models for images and videos across a wide range of real-world applications. However, the traditional approach to understanding diffusion models presents a steep learning curve, as it requires advanced mathematical knowledge in areas such as probability density estimation, stochastic process, and differential equation <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> <d-cite key="luo2022understandingdiffusionmodelsunified"></d-cite>. In this post, we have introduced a new deterministic perspective on diffusion models, enabling us to fully explain and derive them using only a few straightforward mathematical equations. Moreover, we have found that the two methods presented in this post are essentially equivalent to the Denoising Diffusion Implicit Models (DDIMs) method discussed in <d-cite key="song2022denoisingdiffusionimplicitmodels"></d-cite>. However, our methods are derived through a much simpler and more intuitive approach compared to the procedure outlined in <d-cite key="song2022denoisingdiffusionimplicitmodels"></d-cite>.</p>]]></content><author><name>Hui Jiang</name></author><category term="computer-vision"/><category term="machine-learming"/><category term="AI"/><summary type="html"><![CDATA[In this post, we present a deterministic perspective on diffusion models. In this approach, neural networks are trained as an inverse function of the deterministic diffusion mapping that progressively corrupts images at each time step. This method simplifies the derivation of diffusion models, enabling us to fully explain and derive them using only a few straightforward mathematical equations.]]></summary></entry><entry><title type="html">Understanding Transformers and GPT: An In-depth Overview</title><link href="https://incml.github.io/huijiang/blog/2023/Transformer-GPT/" rel="alternate" type="text/html" title="Understanding Transformers and GPT: An In-depth Overview"/><published>2023-03-05T00:00:00+00:00</published><updated>2023-03-05T00:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2023/Transformer-GPT</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2023/Transformer-GPT/"><![CDATA[<p>Transformers <d-cite key="vaswani2023attentionneed"></d-cite> are a type of neural network architecture designed to transform a sequence of \(T\) input vectors,</p> \[\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T \} \;\;\;\;\;(\mathbf{x}_i \in \mathbb{R}^d, \; \forall i=1,2,\cdots,T),\] <p>into an equal-length sequence of the so-called context-dependent output vectors:</p> \[\{ \mathbf{y}_1, \mathbf{y}_2, \cdots, \mathbf{y}_T \} \;\;\;\;\;(\mathbf{y}_i \in \mathbb{R}^h, \; \forall i=1,2,\cdots,T).\] <p>The output sequence in a transformer model is referred to as <em>context-dependent</em> because each output vector is influenced not only by the corresponding input vector but also by the context of the entire input sequence. Specifically, each output vector \(\mathbf{y}_i\) depends on all input vectors in the sequence, not just \(\mathbf{x}_i\) at the same position. As a result, each output vector can be viewed as a representation of not only the input vector at the same location but also its contextual information in the entire sequence.</p> <p>More importantly, transformers utilize a flexible attention mechanism that enables them to generate each output vector \(\mathbf{y}_i\) in a way that mainly relies on a certain number of the most relevant input vectors from anywhere in the input sequence, rather than just those input vectors near the position \(i\) . This ability to selectively attend to relevant information in the input sequence allows transformers to capture long-range dependencies and contextual information, making them a powerful tool for natural language processing and other sequential data tasks.</p> <h2 id="transformers-forward-pass"><strong>Transformers: Forward Pass</strong></h2> <p>Let’s pack all input vectors as a \(d \times T\) matrix, and all output vectors as an \(l \times T\) matrix, as follows:</p> \[\mathbf{X} = \bigg[ \mathbf{x}_1 \; \mathbf{x}_2 \; \cdots \; \mathbf{x}_T \bigg]_{d \times T}\] \[\mathbf{Y} = \bigg[ \mathbf{y}_1 \; \mathbf{y}_2 \; \cdots \; \mathbf{y}_T \bigg]_{h \times T}\] <p>In this way, a transformer can be viewed as a function \(\cal{T}\) that maps from \(\mathbf{X}\) to \(\mathbf{Y}\):</p> \[\cal{T}: \;\;\; \mathbf{X} \longrightarrow \mathbf{Y}\] <p>In the following, we will investigate all steps in the above mapping in a transformer.</p> <ul> <li><strong>Forward step 1:</strong> we first introduce three parameter matrices \(\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{h \times d}\), which transform each input vector \(\mathbf{x}_i\) to generate the so-called <em>query</em> vector \(\mathbf{q}_i\), <em>key</em> vector \(\mathbf{k}_i\), and <em>value</em> vector \(\mathbf{v}_i\):</li> </ul> \[\mathbf{q}_i = \mathbf{A} \mathbf{x}_i, \; \mathbf{k}_i = \mathbf{B} \mathbf{x}_i, \; \mathbf{v}_i = \mathbf{C} \mathbf{x}_i \;\;\;\;(\forall i =1,2, \cdots,T)\] <p>When the <em>query</em>, <em>key</em>, and <em>value</em> vectors are all derived from a common input source, we refer to the transformer’s mechanism as performing <em>self-attention</em>. Conversely, if these vectors are derived from different sources, the mechanism is called <em>cross-attention</em>.</p> <p>The above operations can be combined as three matrix multiplications in the following:</p> \[\mathbf{Q} = \mathbf{A} \mathbf{X}, \;\; \mathbf{K} = \mathbf{B} \mathbf{X}, \;\; \mathbf{V} = \mathbf{C} \mathbf{X}\] <p>where \(\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{h \times T}\) are constructed by lining up the above vectors \(\mathbf{q}_i\), \(\mathbf{k}_i\) and \(\mathbf{v}_i\) column by column.</p> <ul> <li><strong>Forward step 2:</strong> use the above <em>query</em> and <em>key</em> vectors to compute all pair-wise attention between any two input vectors \(\mathbf{x}_i\) and \(\mathbf{x}_t\) (\(\forall i,t =1,2, \cdots, T\)) as follows:</li> </ul> \[c_{it} = \frac{\mathbf{q}_i^\intercal \mathbf{k}_t}{\sqrt{h}} \;\;\;\;\;\;\;\; (\forall i,t =1,2, \cdots, T)\] <p>Next, we normalize each \(c_{it}\) with respect to all \(i\) using the <em>softmax</em> function as follows:</p> \[a_{it} = \frac{e^{c_{it}}}{\sum_{j=1}^T \; e^{c_{jt}}} \;\;\;\;\;\;\;\;(\forall i,t =1,2, \cdots, T)\] <p>We can pack all operations in this step into the following matrix operation:</p> \[\mathcal{A} = \textrm{softmax}\big(\mathbf{Q}^\intercal \mathbf{K}/\sqrt{h} \big) \;\;\;\;\;\; (\mathcal{A} \in \mathbb{R}^{T \times T})\] <p>where the <em>softmax</em> operation is applied to the underlying matrix column-wise.</p> <ul> <li><strong>Forward step 3:</strong> use the above $a_{it}$ as the attention coeffificents to generate an context-dependent vector from all <em>value</em> vectors:</li> </ul> \[\mathbf{z}_t = \sum_{i=1}^T \; a_{it} \mathbf{v}_i \;\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] <p>We can represent the above as the following matrix multiplication:</p> \[\mathbf{Z} = \mathbf{V} \mathcal{A} = \mathbf{V} \; \textrm{softmax}\big(\mathbf{Q}^\intercal \mathbf{K}/\sqrt{h} \big)\] <p>When transformers are employed as decoders to produce tokens, we typically utilize a form of attention known as causal attention. This attention mechanism ensures that each output vector is influenced solely by the input vectors that precede it, rather than any vectors that appear later in the sequence. In this case, we generate each $\mathbf{z}_t$ as</p> \[\mathbf{z}_t = \sum_{i=1}^t \; a_{it} \mathbf{v}_i \;\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] <p>To compute causal attention in matrix form, an upper-triangular matrix is employed to mask the attention matrix $\mathcal{A}$. This masking ensures that the attention mechanism only attends to previous positions in the sequence, as represented by the upper-triangular elements of the attention matrix, while ignoring future positions represented by the lower-triangular elements.</p> <ul> <li><strong>Forward step 4:</strong> apply the layer normalization and one layer of fully-connected feedforward neural network to each $\mathbf{z}_t$ to generate the final output vector $\mathbf{y}_t$ as follows (note that residual connections <d-cite key="he2015deepresiduallearningimage"></d-cite> are introduced here to facilitate optimization during learning):</li> </ul> \[\bar{\mathbf{z}}_t = \mathbf{x}_t + \textrm{LN}_{\mathbf{X} + \gamma,\beta} \big( \mathbf{z}_t \big)\;\;\;\;\;\;\;(\forall t = 1,2,\cdots,T)\] \[\mathbf{y}_t = \bar{\mathbf{z}}_t + \textrm{feedforward} \big( \bar{\mathbf{z}}_t \big) = \bar{\mathbf{z}}_t + \mathbf{W}_2 \; \textrm{ReLU}( \mathbf{W}_1 \bar{\mathbf{z}}_t ) \;\;\;(\forall t = 1,2,\cdots,T)\] <p>where two more parameter matrices $\mathbf{W}_1 \in \mathbb{R}^{h’ \times h}$ and $\mathbf{W}_2 \in \mathbb{R}^{h \times h’}$ are introduced here. For convenience, we can use the following compact matrix form to represent all operations in this step:</p> \[\mathbf{Y} = \mathbf{X} + \textrm{LN}_{\gamma,\beta} \big( \mathbf{Z} \big) +\textrm{feedforward} \Big( \mathbf{X} + \textrm{LN}_{\gamma,\beta} \big( \mathbf{Z} \big)\Big).\] <p>In summary, we can illustrate all attention operations in the forward pass of a transformer using matrices as in Figure 1.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-480.webp 480w,/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-800.webp 800w,/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/2023-03-05-Transformer-GPT/transformer_attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. An illustration of all attention operations (steps 1, 2 and 3) in a transformer. </div> <h2 id="transformers-backward-pass"><strong>Transformers: Backward Pass</strong></h2> <p>Let us now examine how to perform the backward pass to propagate errors in a transformer as well as how to compute the gradients of all transformer parameter matrices, specifically for all attention operations illustrated in Figure 1. For the backward pass of layer normalization and fully-connected feedforward layer, readers are directed to section 8.3.2 in reference <d-cite key="JiangMLF2021"></d-cite>.</p> <p>Assuming we possess error signals for the transformer outputs with respect to a particular objective function $F(\cdot)$, these signals are given as follows:</p> \[\mathbf{e}_t \overset{\Delta}{=} \frac{\partial F}{ \partial \mathbf{z}_t} \;\;\;\;\;\; (\forall t = 1, 2, \cdots, T)\] <p>Arrange them column by column as a matrix:</p> \[\mathbf{E} = \frac{\partial F}{\partial \mathbf{Z}} = \left[ \;\; \frac{\partial F}{ \partial \mathbf{z}_t} \;\; \right]_{h \times T}\] <p>Let’s break down all attention operations in a transformer in Figure 1 step by step backwards from the output towards input as follows:</p> <ul> <li><strong>Backward step 1:</strong> we have</li> </ul> \[\mathbf{z}_t = \sum_{i=1}^T a_{it} \, \mathbf{v}_i\;\;\;\;\;\;\;\;\;\; (\forall t = 1,2, \cdots, T)\] <p>According to the chain rule, we can compute</p> \[\frac{\partial F}{\partial a_{it} } = \frac{\partial F}{\partial \mathbf{z}_t } \frac{\partial \mathbf{z}_t}{ \partial a_{it}} = \mathbf{v}_i^\intercal \frac{\partial F}{\partial \mathbf{z}_t } \;\;\;\;\;(\forall i,t=1,2,\cdots,T)\] <p>Align all of these ($T^2$ terms in total) as a $T \times T$ matrix:</p> \[\left[ \;\; \frac{\partial F}{\partial a_{it}} \;\; \right]_{T \times T } = \bigg[ \;\; \mathbf{V}^\intercal \;\; \bigg]_{T\times h } \bigg[ \;\; \mathbf{E} \;\; \bigg]_{h \times T }\] <ul> <li><strong>Backward step 2:</strong> we normalize as $a_{it} = \frac{e^{c_{i}t}}{\sum_{j=1}^T e^{c_{jt}}} \;\;\;\;\;\; (\forall i,t = 1,2, \cdots, T)$.</li> </ul> <p>We denote</p> \[\mathbf{a}_t \overset{\Delta}{=} \big[ a_{1t} \, a_{2t} \, \cdots, a_{Tt} \big]^\intercal \;\;\;\;\; \textrm{and} \;\;\;\; \mathbf{c}_t \overset{\Delta}{=} \big[ c_{1t} \, c_{2t} \, \cdots, c_{Tt} \big]^\intercal\] \[\frac{\partial F}{\partial \mathbf{a}_t } \overset{\Delta}{=} \big[ \frac{\partial F}{\partial a_{1t}} \, \frac{\partial F}{\partial a_{2t}} \, \cdots \, \frac{\partial F}{\partial a_{Tt}} \big]^\intercal \;\;\;\;\; \textrm{and} \;\;\;\; \frac{\partial F}{\partial \mathbf{c}_t } \overset{\Delta}{=} \big[ \frac{\partial F}{\partial c_{1t}} \, \frac{\partial F}{\partial c_{2t}} \, \cdots \, \frac{\partial F}{\partial c_{Tt}} \big]^\intercal\] <p>According to Eq.(8.14) on page 180 in $[3]$, for any $t=1,2,\cdots,T$, we have</p> \[\frac{\partial F}{\partial \mathbf{c}_t } = \mathbf{J}_{\tiny sm}(t) \; \frac{\partial F}{\partial \mathbf{a}_t } \;\;\;\;\; (\forall t=1,2,\cdots,T)\] <p>with $\mathbf{J}_{\tiny sm}(t) = \textrm{diag} \big( \mathbf{a}_t \big) - \mathbf{a}_t \mathbf{a}_t^\intercal$. Furthermore, we use vector inner products to simplify the above matrix multiplications as follows:</p> \[\frac{\partial F}{\partial \mathbf{c}_t } = \mathbf{J}_{\tiny sm}(t) \; \frac{\partial F}{\partial \mathbf{a}_t } = \Big( \textrm{diag} \big( \mathbf{a}_t \big) - \mathbf{a}_t \mathbf{a}_t^\intercal \Big) \; \frac{\partial F}{\partial \mathbf{a}_t } = \mathbf{a}_t \odot \frac{\partial F}{\partial \mathbf{a}_t } - \big ( \mathbf{a}_t^\intercal \frac{\partial F}{\partial \mathbf{a}_t } \big)\mathbf{a}_t \;\;\;(\forall t=1,2,\cdots,T)\] <p>where $\odot$ indicates element-wise multiplication of two vectors. Next, we align the above results column by column for all $t=1,2,\cdots, T$ and use the notation $\otimes$ to indicate the batch of all $T$ above operations as follows:</p> \[\left[ \;\;\;\; \frac{\partial F}{\partial \mathbf{c}_t } \;\;\;\; \right]_{T \times T } = \mathcal{A} \otimes \left[ \;\;\;\; \frac{\partial F}{\partial \mathbf{a}_t} \;\;\;\; \right]_{T \times T }\] <p>It is worth noting that the aforementioned backward implementation can be applied directly to <em>causal attention</em> without any modifications.</p> <ul> <li><strong>Backward step 3:</strong> due to $c_{it} = \mathbf{q}_i^\intercal \mathbf{k}_t/\sqrt{h} \;\;\;\;\;\; (\forall i,t = 1,2,\cdots, T)$, we have</li> </ul> \[\frac{\partial F}{ \partial \mathbf{q}_i} = \sum_{t=1}^T \frac{\partial F}{\partial c_{it} } \frac{\partial c_{it}}{\partial \mathbf{q}_i} = \frac{1}{\sqrt{h}}\sum_{t=1}^T \frac{\partial F}{\partial c_{it} } \mathbf{k}_t\] <p>Align these vectors column by column as the following matrix format:</p> \[\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} = \frac{1}{\sqrt{h}} \bigg[ \;\; \mathbf{K} \;\; \bigg]_{h \times T} \left[ \;\; \frac{\partial F}{\partial \mathbf{c}_t } \;\; \right]^\intercal_{T \times T }\] <ul> <li><strong>Backward step 4:</strong> because of $\mathbf{q}_i = \mathbf{\mathbf{A}} \mathbf{x}_i$, we have</li> </ul> \[\frac{\partial F}{\partial \mathbf{A}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{q}_i} \frac{\partial \mathbf{q}_i}{\partial \mathbf{A}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{q}_i} \mathbf{x}_i^\intercal = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] <p>Putting all the above steps together, we have</p> \[\frac{\partial F}{\partial \mathbf{A}} = \frac{1}{\sqrt{h}}\bigg[ \;\; \mathbf{K} \;\; \bigg]_{h \times T} \Bigg( \mathcal{A} \otimes \Bigg( \bigg[ \;\; \mathbf{V}^\intercal \;\; \bigg]_{T\times h } \bigg[ \;\; \mathbf{E} \;\; \bigg]_{h \times T } \Bigg) \Bigg)^\intercal \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] \[= \frac{1}{\sqrt{h}} \mathbf{K} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg)^\intercal \mathbf{X}^\intercal\] <p>Similarly, we can derive</p> \[\frac{\partial F}{\partial \mathbf{B}} = \frac{1}{\sqrt{h}} \mathbf{Q} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg) \mathbf{X}^\intercal\] <p>Since we have</p> \[\mathbf{z}_t = \sum_{i=1}^T a_{it} \mathbf{v}_i\;\; (\forall t = 1,2, \cdots, T)\] <p>We compute</p> \[\frac{\partial F}{ \partial \mathbf{v}_i} = \sum_{t=1}^T a_{it} \frac{\partial F}{\partial \mathbf{z}_t}\] <p>Arrange them column by column into a matrix</p> \[\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\;\bigg]_{h \times T} = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{z}_i} \;\;\bigg]_{h \times T} \bigg[ \;\; a_{it} \;\; \bigg]^\intercal_{T \times T} = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{z}_i} \;\;\bigg]_{h \times T} \mathcal{A}^\intercal = \mathbf{E} \mathcal{A}^\intercal\] <p>Since we have $\mathbf{v}_i = \mathbf{C} \mathbf{x}_i \;\; (\forall i=1,2,\cdots,T)$ , we compute</p> \[\frac{\partial F}{\partial \mathbf{C}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{v}_i} \frac{\partial \mathbf{v}_i}{\partial \mathbf{C}} = \sum_{i=1}^T \frac{\partial F}{\partial \mathbf{v}_i} \mathbf{x}_i^\intercal = \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\; \bigg]_{h \times T} \bigg[ \;\; \mathbf{X}^\intercal \;\; \bigg]_{T\times d}\] <p>As a result, we have</p> \[\frac{\partial F}{\partial \mathbf{C }} = \mathbf{E} \mathcal{A}^\intercal \mathbf{X}^\intercal\] <p>Finally, we back-propagate the error signals from output to input. The input $\mathbf{X}$ affects the output through three different paths, i.e. $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$. Therefore, we have</p> \[\frac{\partial F}{\partial \mathbf{X}} = \frac{\partial F}{\partial \mathbf{Q}} \frac{\partial \mathbf{Q}}{ \partial \mathbf{X}} + \frac{\partial F}{\partial \mathbf{K}} \frac{\partial \mathbf{K}}{ \partial \mathbf{X}} + \frac{\partial F}{\partial \mathbf{V}} \frac{\partial \mathbf{V}}{ \partial \mathbf{X}}\] \[= \bigg[ \;\; \mathbf{A}^\intercal \;\; \bigg]_{d\times h} \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{q}_i} \;\; \bigg]_{h \times T} + \bigg[ \;\; \mathbf{B}^\intercal \;\; \bigg]_{d\times h} \bigg[ \;\; \frac{\partial F}{ \partial \mathbf{k}_i} \;\; \bigg]_{h \times T} + \bigg[ \;\; \mathbf{C}^\intercal \;\; \bigg]_{d\times l}\bigg[ \;\; \frac{\partial F}{ \partial \mathbf{v}_i} \;\; \bigg]_{h \times T}\] \[= \frac{1}{\sqrt{h}} \bigg( \mathbf{A}^\intercal \mathbf{K} \Big( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \Big)^\intercal + \mathbf{B}^\intercal \mathbf{Q} \Big( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \Big) \bigg) + \mathbf{C}^\intercal \mathbf{E} \mathcal{A}^\intercal\] <p>Finally, we summarize the above results using a more compact matrix representation. If we define the following $3h \times T$ matrix:</p> \[\mathbf{P} \overset{\Delta}{=} \begin{bmatrix} \frac{1}{\sqrt{h}} \mathbf{K} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg)^\intercal \\ \frac{1}{\sqrt{h}} \mathbf{Q} \bigg( \mathcal{A} \otimes \big( \mathbf{V}^\intercal \mathbf{E} \big) \bigg) \\ \mathbf{E} \mathcal{A}^\intercal \end{bmatrix}_{ 3h \times T}\] <p>we have</p> \[\begin{bmatrix} \frac{\partial F}{\partial \mathbf{A}} \\[0.1cm] \frac{\partial F}{\partial \mathbf{B}} \\[0.1cm] \frac{\partial F}{\partial \mathbf{C}} \end{bmatrix} = \mathbf{P} \, \mathbf{X}^\intercal \;\;\;\;\;\;\Big(\in \mathbb{R}^{3h \times d} \Big)\] \[\frac{\partial F}{\partial \mathbf{X}} = \begin{bmatrix} \mathbf{A} \\ \mathbf{B} \\ \mathbf{C} \end{bmatrix}^\intercal \, \mathbf{P} = \Big[ \mathbf{A}^\intercal \;\; \mathbf{B}^\intercal \;\; \mathbf{C}^\intercal \Big] \, \mathbf{P} \;\;\;\;\;\;\Big(\in \mathbb{R}^{d \times T} \Big)\] <p><em>Note: Refer to <a href="https://colab.research.google.com/drive/1-op--04cwJI2L8iPNIgNoPsJ3uPke8OB">a pytorch implementation at Colab</a> and its comparison with pytorch autograd (or a <a href="https://colab.research.google.com/drive/1WOdo0AuSn-lIzOxXLxTwDAKPJhzEvJ2e">JAX implementation at Colab</a>).</em></p> <h2 id="gpt-3"><strong>GPT-3</strong></h2> <p>The authors of <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite> propose a deep multi-head transformer structure named <em>GPT-3</em> for processing text data. In GPT-3, the values of $d$, $h$, and $T$ are set to $d=12288$, $h=128$, and $T=2048$, respectively, while the vocabulary is composed of $50257$ distinct tokens.</p> <p>To begin, <em>GPT-3</em> employs a tokenizer to split any text sequence into a sequence of tokens (each token is a common word fragment). These tokens are then transformed into vectors of $d=12288$ dimensions using a word embedding matrix $\mathbf{W}_0 \in \mathbb{R}^{12288 \times 50257}$. Subsequently, the input sequence $\mathbf{X} \in \mathbb{R}^{12288 \times 2048}$ is transformed into $\mathbf{Y} \in \mathbb{R}^{12288 \times 2048}$ using 96 layers of multi-head transformer blocks. Each block is defined as follows:</p> <p><strong>(1)</strong> Each multi-head transformer in <em>GPT-3</em> uses $96$ heads:</p> \[\mathbf{A}^{(j)}, \mathbf{B}^{(j)}, \mathbf{C}^{(j)} \in \mathbb{R}^{128 \times 12288} \;\;\; (j=1,2, \cdots, 96)\] <p>which compute for all $j=1,2,\cdots,T$:</p> \[\mathbf{Z}^{(j)} \in \mathbb{R}^{ 128 \times 2048} = \big( \mathbf{C}^{(j)} \mathbf{X} \big) \; \textrm{softmax}\Big( \big(\mathbf{A}^{(j)} \mathbf{X} \big)^\intercal \big( \mathbf{B}^{(j)} \mathbf{X} \big)/\sqrt{128}\Big)\] <p><strong>(2)</strong> Concatenate the outputs from all heads:</p> \[\mathbf{Z} \in \mathbb{R}^{12288 \times 2048} = \mathbf{W}^o \textrm{concat}\big(\mathbf{Z}^{(1)}, \cdots, \mathbf{Z}^{(96)}\big)\] <p>where $\mathbf{W}^o \in \mathbb{R}^{12288\times12288}$.</p> <p><strong>(3)</strong> Apply layer normalization to each column of $\mathbf{Z}$: $\mathbf{z}_t \in \mathbb{R}^{12288} \; (\forall t=1,2,\cdots,2048)$ as</p> \[\bar{\mathbf{z}}_t = \mathbf{x}_t + \textrm{LN}_{\gamma,\beta} \big(\mathbf{z}_t \big)\] <p><strong>(4)</strong> Apply nonlinearity to each column as:</p> \[\mathbf{y}_t= \bar{\mathbf{z}}_t + \textrm{feedforward} \big( \bar{\mathbf{z}}_t\big) = \bar{\mathbf{z}}_t + \mathbf{W}_2 \textrm{ReLU} (\mathbf{W_1} \bar{\mathbf{z}}_t)\] <p>where $\mathbf{W}_1 \in \mathbb{R}^{49152 \times 12288}$, and $\mathbf{W}_2 \in \mathbb{R}^{12288 \times 49152}$.</p> <p>Based on these, we may calculate that the total number of parameters in <em>GTP-3</em> is about $175$ billions.</p> <p>During training, a sequence of training vectors</p> \[\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_{2048} \}\] <p>is fed into <em>GPT-3</em> as input $\mathbf{X} \in \mathbb{R}^{12288\times 2048}$. For each time step $t=1,2,\cdots,2047$, <em>GPT-3</em> is trained to predict the token at position $t+1$ based on all vectors appearing up to position $t$, i.e., ${\mathbf{x}_1, \cdots, \mathbf{x}_t}$.</p> <p>After its training, GPT-3 has the ability to create new sequences by using an input sequence as a prompt. To do this, the model calculates the probabilities of the possible next tokens that could follow the given prompt, and then selects a new token by randomly sampling from these probabilities. The selected token is then added to the end of the prompt, forming a new prompt. This process continues until the model generates a termination token.</p>]]></content><author><name>Hui Jiang</name></author><category term="machine-learming"/><category term="AI"/><category term="NLP"/><summary type="html"><![CDATA[In this post, we delve into the technical details of the widely used transformer architecture by deriving all formulas involved in its forward and backward passes step by step. By doing so, we can implement these passes ourselves and often achieve more efficient performance than using autograd methods. Additionally, we introduce the technical details on the construction of the popular GPT-3 model using the transformer architecture.]]></summary></entry><entry><title type="html">Machine Learning Fundamentals</title><link href="https://incml.github.io/huijiang/blog/2022/Machine-Learning-Fundamentals/" rel="alternate" type="text/html" title="Machine Learning Fundamentals"/><published>2022-01-21T01:00:00+00:00</published><updated>2022-01-21T01:00:00+00:00</updated><id>https://incml.github.io/huijiang/blog/2022/Machine-Learning-Fundamentals</id><content type="html" xml:base="https://incml.github.io/huijiang/blog/2022/Machine-Learning-Fundamentals/"><![CDATA[<div class="row"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/huijiang/assets/img/mlb-book-cover-small-480.webp 480w,/huijiang/assets/img/mlb-book-cover-small-800.webp 800w,/huijiang/assets/img/mlb-book-cover-small-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/huijiang/assets/img/mlb-book-cover-small.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This post provides complementary materials for my recent textbook <a href="https://www.amazon.com/Machine-Learning-Fundamentals-Concise-Introduction/dp/1108940021"><strong>Machine Learning Fundamentals</strong></a> by <a href="https://incml.github.io/huijiang/"><em>Hui Jiang</em></a>, Cambridge University Press, 2021.</p> <h3 id="slides-per-chapter-detailed--contents-is-here">Slides per chapter (Detailed contents is <a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:detailedcontents.pdf">here</a>)</h3> <ul> <li><strong>Ch 1: Introduction</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch1_introduction.pdf">slides</a>)</li> <li><strong>Ch 2: Mathematical Foundation</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch2_mathematical_foundation.pdf">slides</a>)</li> <li><strong>Ch 3: Supervised Machine Learning (in a nutshell)</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch3_supervised_machine_learning.pdf">slides</a>)</li> <li><strong>Ch 4: Feature Extraction</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch4_feature_extraction.pdf">slides</a>)</li> <li><strong>Ch 5: Statistical Learning Theory</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch5_statistical_learning_theory.pdf">slides</a>)</li> <li><strong>Ch 6: Linear Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch6_linear_models.pdf">slides</a>)</li> <li><strong>Ch 7: Learning Discriminative Models in General</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch7_learning_discriminative_models.pdf">slides</a>)</li> <li><strong>Ch 8: Neural Networks</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch8_neural_networks.pdf">slides</a>)</li> <li><strong>Ch 9: Ensemble Learning</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch9_ensemble_learning.pdf">slides</a>)</li> <li><strong>Ch 10: Overview of Generative Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch10_overview_generative_models.pdf">slides</a>)</li> <li><strong>Ch 11: Unimodal Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch11_unimodal_models.pdf">slides</a>)</li> <li><strong>Ch 12: Mixture Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch12_mixture_models.pdf">slides</a>)</li> <li><strong>Ch 13: Entangled Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch13_entangled_models.pdf">slides</a>)</li> <li><strong>Ch 14: Bayesian Learning</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch14_bayesian_learning.pdf">slides</a>)</li> <li><strong>Ch 15: Graphical Models</strong> (<a href="https://wiki.eecs.yorku.ca/user/hj/_media/research:ch15_graphical_models.pdf">slides</a>)</li> </ul> <h3 id="lab-projects-using-jupyter-notebooks">Lab Projects (using Jupyter Notebooks)</h3> <ul> <li><strong>Lab 0</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab0_Preparation.pdf">Preparation</a> (available at <a href="https://colab.research.google.com/drive/1nB-uULhbpFTqlUOekEW3272eqKJg6qok?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab0_Preparation.ipynb">ipynb</a>)</li> <li><strong>Lab 1</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab1_Data_Visualization.pdf">Data Visualization</a> (available at <a href="https://colab.research.google.com/drive/1zE2OrdJNkmzzn30CdDsj1y9qz585-afF?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab1_Data_Visualization.ipynb">ipynb</a>)</li> <li><strong>Lab 2</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab2_Linear_Regression.pdf">Linear Regression</a> (available at <a href="https://colab.research.google.com/drive/1Ix04T8y0evnFnXWvHHWu0KfqsOA48hUg?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab2_Linear_Regression.ipynb">ipynb</a>)</li> <li><strong>Lab 3</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab3_Logistic_Regression.pdf">Logistic Regression</a> (available at <a href="https://colab.research.google.com/drive/1qUi2qgwBMmHJb7Bx7ca_braGtptmzFOQ?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab3_Logistic_Regression.ipynb">ipynb</a>)</li> <li><strong>Lab 4</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab4_Support_Vector_Machine.pdf">Support Vector Machines</a> (available at <a href="https://colab.research.google.com/drive/1vlN46Xhfv4ES8jFWWI0iTj3WHKjb0KMg?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab4_Support_Vector_Machine.ipynb">ipynb</a>)</li> <li><strong>Lab 5</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab5_Fully_Connected_Neural_Networks.pdf">Fully-Connected Neural Networks</a> (available at <a href="https://colab.research.google.com/drive/1TY3fAAS18lyGrlEsvLhaYm6fIBP7CDgD?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab5_Fully_Connected_Neural_Networks.ipynb">ipynb</a>)</li> <li><strong>Lab 6</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab6_Convolutional_Neural_Networks.pdf">Convolutional Neural Networks</a> (available at <a href="https://colab.research.google.com/drive/1Zkeujeoh4jNR9bQ67kfuIE7cV6KifZS2?usp=sharing">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab6_Convolutional_Neural_Networks.ipynb">ipynb</a>)</li> <li><strong>Lab 7</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab7_Transformers.pdf">Transformers</a> (available at <a href="https://colab.research.google.com/drive/1RvaBdX-KrgCNkVoo39UYqH1qzd-hAT5q">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab7_Transformers.ipynb">ipynb</a>)</li> <li><strong>Lab 8</strong>: <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/pdf/Lab8_Matrix_Factorization.pdf">Matrix Factorization</a> (available at <a href="https://colab.research.google.com/drive/1JaGmlkrC03URmnfkKtOakckP_ds20-WA">Colab</a>, <a href="https://github.com/iNCML/MachineLearningBook/blob/master/labs/ipynb/Lab8_Matrix_Factorization.ipynb">ipynb</a>)</li> <li><strong>Lab 9</strong>: Decision Trees, Random Forests and Boosted Trees</li> <li><strong>Lab 10</strong>: Gaussian Classifiers and Gaussian Mixture Models</li> </ul> <h3 id="citation-bibtex">Citation (bibtex):</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@book{Jiang-MLF-2021, 
  author = {Hui Jiang},
  title = {Machine Learning Fundamentals}, 
  publisher = {Cambridge University Press},
  year = {2021} 
}
</code></pre></div></div>]]></content><author><name></name></author><category term="machine-learming"/><category term="AI"/><summary type="html"><![CDATA[Supplementary materials for the book "Machine Learning Fundamentals", published by Cambridge University Press.]]></summary></entry></feed>